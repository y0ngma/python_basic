{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9.자연어처리-전처리.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIHMTO2CgnjX",
        "colab_type": "text"
      },
      "source": [
        "# 코퍼스(말뭉치) 수집\n",
        "\n",
        "- 데이터를 제공받는다, 크롤링등을 통해서 수집한다\n",
        "- 데이터 수집 단계와 동일\n",
        "- 데이터\n",
        "  - 단일 언어 코퍼스 (한개 언어로만 구성)\n",
        "  - 이중 언어 코퍼스 (2개 언어로만 구성)\n",
        "  - 다중 언어 코퍼스 (2개이상 언어로 구성)\n",
        "  - 병렬 언어 코퍼스 (여러개의 언어가 쌍으로(대칭적으로) 구성)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvsZJRKei5BR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBC_HoiJgvmO",
        "colab_type": "text"
      },
      "source": [
        "# 정제\n",
        "\n",
        "- 전각 문자(특수문자표..) 제거 (검토, ,.?!등은 고려)\n",
        "- 대소문자 통일(알파벳기반)\n",
        "- 정규식을 이용(노이즈 제거)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhDcIbvSgvo4",
        "colab_type": "text"
      },
      "source": [
        "# 문장 단위 분절\n",
        "- 하나의 문장은(문장의 끝은 .) 재료에 따라 다양하게 존재한다\n",
        "  - 하나의 문장이 여러줄에 걸쳐있다\n",
        "  - 한줄에 여러문장이 존재한다  \n",
        "  => 이런 재료를 한줄에 한문장으로 구성되게 조정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAlnAN6Bfp5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 설치\n",
        "#! pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_HYyajofv8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7J0zc-QgMrT",
        "colab_type": "code",
        "outputId": "4597c2d7-6fd1-4943-a26a-d55940f39b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# 한줄에 여러문장, 여러줄\n",
        "src_text = '''초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 \"기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것\"이다. 그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다. 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다. 대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.\n",
        "초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 \"기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것\"이다. 그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다. 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다. 대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.\n",
        "\n",
        "'''\n",
        "print( '[%s]' % src_text )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 \"기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것\"이다. 그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다. 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다. 대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.\n",
            "초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 \"기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것\"이다. 그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다. 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다. 대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.\n",
            "\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtydEEM-fv-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 작업 편의상 파일에 기록했다\n",
        "with open('test.txt','w') as f:\n",
        "  f.write( src_text )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfe5wPhffwBQ",
        "colab_type": "code",
        "outputId": "21734b73-6b38-4ccd-97f2-778421d2eeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 문장의 토큰화처리\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLhW8B6nfwDn",
        "colab_type": "code",
        "outputId": "4501bcc3-b0d3-42c8-ef43-5f43dd983540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "# 한글의 경우 정형화된 문서(법률, 소설, 기사 등등은 쉼표, 마침표를 잘 찍어서 표현)이외에\n",
        "# 대화체(sns, 채팅등 문서)는 누락이 많아서 크게 효과는 못 얻을수 있다. \n",
        "# 단, sns, 채팅 => 한 문장 단위로 끊어치는 성향이 강해서 그 기준으로 분류해도 좋다\n",
        "with open('test.txt','r') as f:\n",
        "  for line in f:\n",
        "    msg = line.strip()# 한줄씩 공백제거해서 획득\n",
        "    #print(msg)\n",
        "    if msg:\n",
        "      sentences = sent_tokenize( msg ) # 한줄에 대한 문장을 추출 \n",
        "      for s in sentences: # 문장별로 출력\n",
        "        if s:\n",
        "          print( s )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 \"기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것\"이다.\n",
            "그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다.\n",
            "인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다.\n",
            "대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.\n",
            "초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 \"기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것\"이다.\n",
            "그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다.\n",
            "인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다.\n",
            "대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdZ0G23ffwGK",
        "colab_type": "code",
        "outputId": "842649ea-2872-42f4-9414-0cfb8747f036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# 한문장에 여러줄에 걸쳐서 표현되어 있다\n",
        "src_text = '''초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 \n",
        "제안한 것으로 \"기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것\"이다. \n",
        "그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에\n",
        " 대한 고려를 하지 못한 것 같다. \n",
        " 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다. \n",
        " 대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, \n",
        " 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.\n",
        "'''\n",
        "print( '[%s]' % src_text )\n",
        "\n",
        "with open('test2.txt','w') as f:\n",
        "  f.write( src_text )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 \n",
            "제안한 것으로 \"기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것\"이다. \n",
            "그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에\n",
            " 대한 고려를 하지 못한 것 같다. \n",
            " 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다. \n",
            " 대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, \n",
            " 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SwMYCPukiLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 한 문장이 여러줄에 포함된 경우 => 한줄에 여러문장으로 처리하거나, \n",
        "# 마침표 기준으로 문자을 나누던가 처리가 필요\n",
        "buffer = []\n",
        "with open('test2.txt','r') as f:\n",
        "  for line in f:\n",
        "    msg = line.strip()\n",
        "    buffer += [ msg ]\n",
        "    tmp = ' '.join( buffer )\n",
        "    # 여러줄에 걸친 문장을 한줄로 다시 합치기\n",
        "msg = ' '.join(buffer)\n",
        "# msg를 다시 위에서 처럼 한문장씩 빼주면 처리가 가능"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdJXXeFtgvrQ",
        "colab_type": "text"
      },
      "source": [
        "# 분절\n",
        "\n",
        "- 형태소 분석, 단순 분석등 정규화 수행\n",
        "- Mecab을 많이 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDRVbtPwrTEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 영어 문장의 경우  => 띠어쓰기 기준으로 토큰화 처리가 가능\n",
        "# 방법1 spacy\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI-U7hm0rgy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 재료\n",
        "en_txt = 'Abby is an American sitcom created by Nat Bernstein and Michael Katlin that aired for one season on UPN from January 6, 2003, to March 4, 2003.'\n",
        "en_txt\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCfjojmor5-2",
        "colab_type": "code",
        "outputId": "40e50c6f-d287-4c5b-bbbf-c8c1becc3ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 띠어쓰기 기준으로 토큰화, 각주(, .)등도 토큰화를 처리했다\n",
        "print( [ token for token in spacy_en.tokenizer( en_txt ) ] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Abby, is, an, American, sitcom, created, by, Nat, Bernstein, and, Michael, Katlin, that, aired, for, one, season, on, UPN, from, January, 6, ,, 2003, ,, to, March, 4, ,, 2003, .]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "878vnTAirTHR",
        "colab_type": "code",
        "outputId": "30c2fd9d-442a-4711-886f-628d36153853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 방법2 nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print( word_tokenize(en_txt) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Abby', 'is', 'an', 'American', 'sitcom', 'created', 'by', 'Nat', 'Bernstein', 'and', 'Michael', 'Katlin', 'that', 'aired', 'for', 'one', 'season', 'on', 'UPN', 'from', 'January', '6', ',', '2003', ',', 'to', 'March', '4', ',', '2003', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW7NeuqPi1H6",
        "colab_type": "code",
        "outputId": "b22be60e-efdb-4b5a-c73a-9c4b6469564e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/분절.png', width=500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "/content/분절.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 500
            }
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I36aJAtQtCul",
        "colab_type": "text"
      },
      "source": [
        "## Mecab\n",
        "- 일본어 형태소 분석용 오픈소스로 개발된 소스를 한국어에 맞게 wraping해서 제공\n",
        "- 한글 분절용으로 대표적으로 가장 많이 사용\n",
        "- '은전한닢'이라는 내용을 처음으로 적용하서 한국어 기능이 추가되었다(Mecab Fork 프로젝트)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWdbN2btmx7",
        "colab_type": "text"
      },
      "source": [
        "## 설치\n",
        "- konlpy\n",
        "- open jdk\n",
        "- Mecab 소스 다운로드 빛 컴파일(make)\n",
        "- Mecab-ko 설치\n",
        "- Mecab-ko-dic 설치\n",
        "- Mecab 설치\n",
        "- 설치후 , 런타임이 종료되면 => 사용않됨  \n",
        "  오랜 사용으로 메모리가 꼬여서 사용안될수도 있다\n",
        "  => 런타임 초기화후 => 설치 => 사용\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKaJDGp8uQG4",
        "colab_type": "code",
        "outputId": "e7be52f2-8602-46a6-de50-5fe5e34e7ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "# konlpy 설치\n",
        "!pip install konlpy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.17.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/90/a94a55a58edfd67360fef85894bfb136a2c28b2cc7227d3a44dc508d5900/JPype1-0.7.1-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 43.7MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n",
            "Installing collected packages: tweepy, beautifulsoup4, JPype1, colorama, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-0.7.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agaq0R0YufDD",
        "colab_type": "code",
        "outputId": "e0f07eb4-4dff-4533-c556-1dd37c0499f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "# jdk 설치\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip3 install JPype1-py3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting JPype1-py3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/81/63f5e4202c598f362ee4684b41890f993d6e58309c5d90703f570ab85f62/JPype1-py3-0.5.5.4.tar.gz (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 2.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: JPype1-py3\n",
            "  Building wheel for JPype1-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for JPype1-py3: filename=JPype1_py3-0.5.5.4-cp36-cp36m-linux_x86_64.whl size=2677226 sha256=89aac600186ef6073adbeb41c05c1149081af257e57487abd6d29894260ba714\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/37/1f/1015d908d12a0e9b239543d031fda0cded9823aa1306939541\n",
            "Successfully built JPype1-py3\n",
            "Installing collected packages: JPype1-py3\n",
            "Successfully installed JPype1-py3-0.5.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kE8AGzqufFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuOPBQ9auoRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "셀스크립트로 설치하는 방법\n",
        "! bash <(curl -s https://.../mecab.sh)\n",
        "'''\n",
        "! bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_XhELGouoKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/tmp/')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.1.tar.gz\n",
        "!tar zxfv mecab-0.996-ko-0.9.1.tar.gz\n",
        "os.chdir('/tmp/mecab-0.996-ko-0.9.1')\n",
        "!./configure\n",
        "!make\n",
        "!make check\n",
        "!make install\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvPJxZ7JuoMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/tmp')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz\n",
        "!tar -zxvf mecab-ko-dic-2.0.1-20150920.tar.gz\n",
        "os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')\n",
        "!./autogen.sh\n",
        "!./configure\n",
        "!make\n",
        "# !sh -c 'echo \"dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic\" > /usr/local/etc/mecabrc'\n",
        "!make install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "637-3q18uoPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install mecab-python\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
        "os.chdir('/content/mecab-python-0.996')\n",
        "\n",
        "!python3 setup.py build\n",
        "!python3 setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMyNHIviuoUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 한글은 띠어쓰기를 수행하면, 토큰화 처리시 단어수가 너무 많아진다 => 형태소 단위로 진행\n",
        "# 띠어쓰기도 잘 이행하지 않고, 100% 아직 정형화가 않되 있어서 사용 X\n",
        "from konlpy.tag import Mecab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPDXNtWtufH9",
        "colab_type": "code",
        "outputId": "6c523f77-e203-4396-9003-09e2ad2b9284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 재료\n",
        "ko_sentence = '오늘 점심 무엇을 먹을까'\n",
        "# 형태소 분석기\n",
        "# Mecab의 성분(태그표) = bit.ly/2StqivH\n",
        "tokenizer   = Mecab()\n",
        "# morphs\n",
        "print( tokenizer.morphs(ko_sentence) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['오늘', '점심', '무엇', '을', '먹', '을까']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr-CpHbmufKS",
        "colab_type": "code",
        "outputId": "1b53e8bd-60e1-42e0-8f29-f31734117131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# pos함수로 품사 태깅 수행\n",
        "tokens = tokenizer.pos(  ko_sentence )\n",
        "print( tokens )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('오늘', 'MAG'), ('점심', 'NNG'), ('무엇', 'NP'), ('을', 'JKO'), ('먹', 'VV'), ('을까', 'EC')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-xcuDm_2GSM",
        "colab_type": "code",
        "outputId": "d6c0541c-bf94-4c7c-ac47-6f22bd07bd67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.tagset.get( 'MAG' )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'일반 부사'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GT7tMWUufM1",
        "colab_type": "code",
        "outputId": "2ab5fc0c-7c51-4e5c-d736-beaa07919e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print( [ ( k , tokenizer.tagset.get( v ) ) for k,v in tokens  ] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('오늘', '일반 부사'), ('점심', '일반 명사'), ('무엇', '대명사'), ('을', '목적격 조사'), ('먹', '동사'), ('을까', '연결 어미')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3gyPOpg3C30",
        "colab_type": "text"
      },
      "source": [
        "## Moses\n",
        "- 영어 분절용, 띠어쓰기 문제점은 않가지고 있다\n",
        "- 쉼표, 마침표, 인용부호은 띠어 쓰기 처리 필요\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKTcypt53Rry",
        "colab_type": "code",
        "outputId": "1df14141-76d9-407f-d985-54c27f00914a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "nltk.download('perluniprops')\n",
        "from nltk.tokenize.moses import MosesTokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-8q19Ls3tqJ",
        "colab_type": "code",
        "outputId": "ffcc4973-619d-4c0a-8039-e99c10b5db7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "en_str = '''According to the Book of Exodus, Moses was born in a time when his people, the Israelites, an enslaved minority, were increasing in population and, as a result, the Egyptian Pharaoh worried that they might ally themselves with Egypt's enemies.\n",
        "'''\n",
        "en_str"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"According to the Book of Exodus, Moses was born in a time when his people, the Israelites, an enslaved minority, were increasing in population and, as a result, the Egyptian Pharaoh worried that they might ally themselves with Egypt's enemies.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti7eLl1G3-Qh",
        "colab_type": "code",
        "outputId": "1e17ffd9-f275-4aab-91b3-9492baf0ca8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "nltk.download('nonbreaking_prefixes')\n",
        "t = MosesTokenizer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgbD8zDz3-TN",
        "colab_type": "code",
        "outputId": "eaf82558-44ff-4442-b299-2f68d4eb7a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tmp = t.tokenize( en_str.strip() )\n",
        "print( tmp )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['According', 'to', 'the', 'Book', 'of', 'Exodus', ',', 'Moses', 'was', 'born', 'in', 'a', 'time', 'when', 'his', 'people', ',', 'the', 'Israelites', ',', 'an', 'enslaved', 'minority', ',', 'were', 'increasing', 'in', 'population', 'and', ',', 'as', 'a', 'result', ',', 'the', 'Egyptian', 'Pharaoh', 'worried', 'that', 'they', 'might', 'ally', 'themselves', 'with', 'Egypt', '&apos;s', 'enemies', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSODjw-Zgvv-",
        "colab_type": "text"
      },
      "source": [
        "## 서브워드 분절\n",
        "- BPE 알고리즘 사용\n",
        "- 좀더 작은 단위의 서브워드로 분절\n",
        "  - 전제 : 단어는 의미를 가진 더 작은 서브워드들의 조합으로 이루어진다\n",
        "    - 한국어 : 집중, 정문,.. => 集(모을집) + 中(가운데중)\n",
        "    - 영어 : concentrate => con + cent + rate\n",
        "  - 적용\n",
        "    - 신경망 => 신경 + 망\n",
        "    - 중국어 => 중국 + 어    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGel1B4Bgvtm",
        "colab_type": "text"
      },
      "source": [
        "## 병렬 코퍼스 분절\n",
        "\n",
        "- 여러 문장 단위로 분절"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR1Do_No5r54",
        "colab_type": "text"
      },
      "source": [
        "# 사전(단어장, voca..)생성\n",
        "\n",
        "## 개요\n",
        "- 페이스북에 MUSE에서는 사전 구축 방법을 코드와 함께 제공 \n",
        "- 언어별로 단어가 많이 사용되거나 하면, 사전이 필수적으로 필요하게 된다\n",
        "- 해당 코퍼스를 기준으로 형태소를 뽑아내면(분절) 하면 이를 사전(단어장으로 )으로 구성할수 있다.\n",
        "  - 특정 형태소에 대해 인덱스가 부여가 되고\n",
        "  - 그 인덱스를 기준으로 백터화를 수행할수 있다\n",
        "  - 단어 임베딩 처리를 통해서 인공신경망에서 훈련할수 있는 데이터화 처리가 완성된다\n",
        "\n",
        "## 단어장 생성 방식\n",
        "- 중복되지 않는 토큰(고유하다, 사전내에서는)\n",
        "- 해당 토큰에 고유한 숫자를 부여(인덱스)\n",
        "- 샘플 데이터는 네이버 감성 영화 코퍼스 데이터사용\n",
        "  - https://github.com/e9t/nsmc => ratings.txt 업로드\n",
        "  - 컬럼 3개 : id\tdocument\tlabel\n",
        "  - label은 1이면 긍정, 0이면 부정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLze4BfG8sgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# step 1 : 코퍼스 수집(git를 통해서 네이버 감성 영화 코퍼스 자료 획득)\n",
        "# step 2 : 분절용 한글 처리 형태소 분석기(분절 세부 과정은 통으로 분절이라고 표현)\n",
        "from konlpy.tag import Mecab\n",
        "tokenizer = Mecab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b33cSz389B4B",
        "colab_type": "code",
        "outputId": "6bde54c1-c535-4bdc-9fbb-ff641aca2871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# step 3 : 사전, 단어장 만들기\n",
        "# 3-1. 파일을 읽는다\n",
        "with open('/content/ratings.txt') as f:\n",
        "  # 3-2. 내용을 이미 한줄로 되어서 문장 단위 분절은 필요 없고, 바로 라인별로 분해->id는 누락\n",
        "  # 컬럼 파트는 빼고 사용\n",
        "  raw = f.read().splitlines()[1:]\n",
        "  print( len(raw), raw[:3] )\n",
        "  # 데이터 한개는 \\t 으로 구분되어 있다\n",
        "  # => [ [ '어릴때보고 ..', '1' ], [], .... ]\n",
        "  #data = [ lineStr.split('\\t')[1:] for lineStr in raw ]\n",
        "  data = list( map( lambda x : x.split('\\t')[1:], raw ) )\n",
        "  print( data[:2] )\n",
        "  # 3-3. 토큰화처리, 분절처리 => morphs()\n",
        "  # [ (['','',''], 1),(), (), ... ]\n",
        "  # x => ['어릴때보고 지금다시봐도 재밌어요ㅋㅋ', '1']\n",
        "  # x[1] => 1\n",
        "  # x[0] => '어릴때보고 지금다시봐도 재밌어요ㅋㅋ'\n",
        "  data = list( map( lambda x :( tokenizer.morphs(x[0]), x[1]), data ) )\n",
        "  print( data[:2] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200000 ['8112052\\t어릴때보고 지금다시봐도 재밌어요ㅋㅋ\\t1', '8132799\\t디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.\\t1', '4655635\\t폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.\\t1']\n",
            "[['어릴때보고 지금다시봐도 재밌어요ㅋㅋ', '1'], ['디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.', '1']]\n",
            "[(['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ'], '1'), (['디자인', '을', '배우', '는', '학생', '으로', ',', '외국', '디자이너', '와', '그', '들', '이', '일군', '전통', '을', '통해', '발전', '해', '가', '는', '문화', '산업', '이', '부러웠', '는데', '.', '사실', '우리', '나라', '에서', '도', '그', '어려운', '시절', '에', '끝', '까지', '열정', '을', '지킨', '노라노', '같', '은', '전통', '이', '있', '어', '저', '와', '같', '은', '사람', '들', '이', '꿈', '을', '꾸', '고', '이뤄나갈', '수', '있', '다는', '것', '에', '감사', '합니다', '.'], '1')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Naq1DjGRMVI",
        "colab_type": "code",
        "outputId": "e8a5975c-e3c0-4acc-9807-3ad8d595d276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 데이터가 너무 많으니까, 샘플링을 해서 진행\n",
        "# 긍정 : 1, 부정 : 0 \n",
        "# 긍정 리뷰 10개, 부정 리뷰 10개를 샘플링\n",
        "sampling_data = data[5:5+10] +  data[-10:]\n",
        "print( sampling_data[0], sampling_data[-1] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['사랑', '을', '해', '본', '사람', '이', '라면', '처음', '부터', '끝', '까지', '웃', '을', '수', '있', '는', '영화'], '1') (['포', '풍', '저그', '가', '나가', '신다', '영차영차', '영차'], '0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKzppydWTH4J",
        "colab_type": "code",
        "outputId": "60ec57a8-35d4-463a-e59d-c618ccdd1716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 리스트 구성원의 순서끼리 묶어서(같은 성분끼리 묶어서) 데이터를 구성\n",
        "# zip(*시퀀스데이터)\n",
        "# [ ([],[],[],), (1,1,1,.. 0,0,0) ]\n",
        "print( list(zip( *sampling_data )) )\n",
        "tmp = list(zip( *sampling_data ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['사랑', '을', '해', '본', '사람', '이', '라면', '처음', '부터', '끝', '까지', '웃', '을', '수', '있', '는', '영화'], ['완전', '감동', '입니다', '다시', '봐도', '감동'], ['개', '들', '의', '전쟁', '2', '나오', '나요', '?', '나오', '면', '1', '빠', '로', '보', '고', '싶', '음'], ['굿'], ['바보', '가', '아니', '라', '병', '쉰', '인', '듯'], ['내', '나이', '와', '같', '은', '영화', '를', '지금', '본', '나', '는', '감동', '적', '이', '다', '.', '.', '하지만', '훗날', '다시', '보', '면', '대사', '하나하나', '그', '감정', '을', '완벽', '하', '게', '이해', '할', '것', '만', '같', '다', '.', '..'], ['재밌', '다'], ['고질라', '니무', '귀엽', '다', '능', 'ㅋㅋ'], ['영화', '의', '오페라', '화', '라고', '해야', '할', '작품', '.', '극단', '적', '평갈', '림', '은', '어쩔', '수', '없', '는', '듯', '.'], ['3', '도', '반전', '좋', '았', '제', '^^'], ['장르', '는', '무협', '인데', '내', '가', '보', '기', '엔', '코믹', '이', '던데', '막장', '평점', '2', '점', '도', '아깝', '다'], ['나치', '입장', '에서', '본', '영화', '가', '갑자기', '연속', '으로', '나오', '네', '?', '뭔일', '있', '었', '나', '.', '..'], ['태권도', '?', '??'], ['음', '왜', '봤', '을까', '?', '예고편', '이', '다', '-'], ['개연', '성', '이', '없', '어요', '.', '.', '별루', '다', '.', '..'], ['포켓', '몬스터', '짜', '가', 'ㅡㅡ', ';;'], ['쓰', '.', '레', '.', '기'], ['완전', '사이코', '영화', '.', '마지막', '은', '더욱더', '이', '영화', '의', '질', '을', '떨어트린', '다', '.'], ['왜', '난', '재미없', '었', '지', 'ㅠㅠ', '라따뚜이', '보', '고', '나', '서', '스머프', '봐서', '그런가', 'ㅋㅋ'], ['포', '풍', '저그', '가', '나가', '신다', '영차영차', '영차']), ('1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsv2HnL5SAGb",
        "colab_type": "code",
        "outputId": "4296e912-121b-4bcc-c9cd-adbe3c4e51fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 테스트 토큰화된 데이터만 \n",
        "print( tmp[0][0] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['사랑', '을', '해', '본', '사람', '이', '라면', '처음', '부터', '끝', '까지', '웃', '을', '수', '있', '는', '영화']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XOGfV3xUGKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 사전화 작업은 토큰을 => 고유번호 대입\n",
        "# 전체 데이터를 돌면서 => 하나의 리뷰를 획득 \n",
        "# => 하나의 토큰을 획득 => 사전에 있었는지 체크 => 없었다면(신규단어) => 새로운 인덱스를 부여\n",
        "# => 새로운 인덱스는 어떻게 계산? 딕셔너리(사전의 자료구조)에 길이값으로 세팅하겟다\n",
        "# 사전\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8hA6Aj1VsDw",
        "colab_type": "code",
        "outputId": "627c5065-a1df-4577-cd6f-910757c02272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 전체 데이터 \n",
        "len(tmp[0])\n",
        "vocab = dict() # { '토큰':인덱스 }\n",
        "# 추가부분 \n",
        "# 단어장에 없는 형태소가 있는 경우 : unknow\n",
        "# 문장의 크기를 맞출때 필요할경우 문장의 길이를 동일하게 처리하기 위해 padding 추가\n",
        "# 토치텍스트에서 시퀀스 구성후 문장 임베딩될때 삽입되는 기호와 동일하게 구성\n",
        "# <unk>:0, <pad>:1\n",
        "vocab['<unk>'] = 0\n",
        "vocab['<pad>'] = 1\n",
        "for sentence in tmp[0]:  # 20문장을 반복하면서 한문장씩 뽑고\n",
        "  #print( sentence )\n",
        "  for token in sentence: # 한문장에서 형태소를 한개씩 뽑고\n",
        "    #print( token )\n",
        "    if not token in vocab:# 사전안에 해당 토큰이 등록되어 있는가?없다면\n",
        "      #vocab[token] = len(vocab)\n",
        "      vocab.setdefault( token, len(vocab) )# 사전에 형태소를 추가, 단 값은 사전의 현재 개수\n",
        "len(vocab), vocab['-']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(155, 122)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKNHUrDXXbb1",
        "colab_type": "text"
      },
      "source": [
        "# 단어 임베딩\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvZeBp8DfHzc",
        "colab_type": "text"
      },
      "source": [
        "## 수치화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9SLgE0cbvb1",
        "colab_type": "code",
        "outputId": "679a38da-9eeb-4ddf-b8d0-c4a637ae5a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# ( 한 문장에서 사용된 형태소가 모여있는 리스트, 사전)\n",
        "def numericalize( sent, vocab ):\n",
        "  # sent에서 토큰을 하나씩 꺼내서, 사전에 대조하여 인덱스(토큰의 인덱스값,특성값)를 구한다\n",
        "  # '사랑' => 2번으로 표기\n",
        "  tmp = list()\n",
        "  for token in sent:   # 형태소 덩어리에서 한개의 토큰(형태소)를 추출\n",
        "    # 해당 토큰이 사전에 존재하는가?\n",
        "    if token in vocab: # 존재하면\n",
        "      tmp.append( vocab[token] ) # '사랑' => 2\n",
        "    else:              # 그런단어가 사전에 없다면\n",
        "      tmp.append( vocab.get('<unk>') )\n",
        "  return tmp\n",
        "\n",
        "# [ ([1,2,3,..], 1), (), (), ...]\n",
        "numericalize_data = [ ( numericalize( sent , vocab), label ) \n",
        "                      for sent, label in sampling_data        # 1. 문장(형태소처리된) 20개를 반복\n",
        "                    ]\n",
        "# 개별 확인\n",
        "print(numericalize_data[-2])\n",
        "print(sampling_data[-2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([118, 139, 140, 115, 141, 142, 143, 35, 36, 55, 144, 145, 146, 147, 78], '0')\n",
            "(['왜', '난', '재미없', '었', '지', 'ㅠㅠ', '라따뚜이', '보', '고', '나', '서', '스머프', '봐서', '그런가', 'ㅋㅋ'], '0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SsO1Qwbe14b",
        "colab_type": "code",
        "outputId": "219ec901-18bc-4bb5-80c6-3a1e98e98ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpC4x-LEfU6p",
        "colab_type": "text"
      },
      "source": [
        "## 단어 주머니(Bag of Words)\n",
        "\n",
        "- 고전 기법\n",
        "- 단어의 출현빈도로만 문장별로 기록(순서는 미고려)\n",
        "- 문서-단어의 행렬이라고 표현 : term-matrix\n",
        "- 행렬 구성\n",
        "  - 행:문장\n",
        "  - 열:각 문장에서 사용된 고유 단어(형태소)의 각각의 개수가 표현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcYmGUvTgbOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kueVRsqngbQ9",
        "colab_type": "code",
        "outputId": "b93e0c78-2720-491e-f783-40af09bc2bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 사용데이터는 numericalize_data를 사용한다 => 긍정 10, 부정10개의 문장\n",
        "# 문서(장)의 개수 : 행\n",
        "num_docs = len(numericalize_data)\n",
        "num_docs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPqWRxNLgbTL",
        "colab_type": "code",
        "outputId": "ef8cf9bc-714f-4a5f-e272-5aacf990ed13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 사전의 단어수 : 열\n",
        "num_vocabs = len( vocab )  \n",
        "num_vocabs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_5x1VItgbVW",
        "colab_type": "code",
        "outputId": "8317a5a7-b33f-410e-d835-2cc4474d8b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# 행렬 구성 => 초기값 0\n",
        "term_matrix = torch.zeros( num_docs, num_vocabs )\n",
        "term_matrix,term_matrix.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]), torch.Size([20, 155]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtA8sDQFgbX7",
        "colab_type": "code",
        "outputId": "0f6b1d30-a8eb-409f-ecf5-a6c2bd0dd6a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "# numericalize_data을 반복 작업하면서, 원소들의 카운트를 처리\n",
        "for sent, label in numericalize_data:\n",
        "  print( sent )\n",
        "  # 한 문장안에 특정 토큰이 몇번 나왔는가?\n",
        "  #print( list(Counter(sent).items()) )\n",
        "  # 각 자리 원소끼리 따로 묶어라 => zip(*시퀀스데이터)\n",
        "  # Counter(sent) => { 1:1, 2:2, 3:1, 4:1 } => { 형태소1번(사전상의인덱스):1번등장,  }\n",
        "  # Counter(sent).items() => [ (1, 1), (2, 2), (3, 1), ... ]\n",
        "  tokens, cnts = zip(*list(Counter(sent).items()))\n",
        "  # 문장안에 각 형태소와, 빈도를 가진 각각의 리스트를 획득\n",
        "  # tokens => [ 1,2,3,4,.... ]\n",
        "  # cnts   => [ 1, 2, 1, 1, ...]\n",
        "  # 수치화된 토큰만\n",
        "  print( tokens )\n",
        "  # 빈도만\n",
        "  print( cnts )\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ecce391b3162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumericalize_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# 한 문장안에 특정 토큰이 몇번 나왔는가?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#print( list(Counter(sent).items()) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# 각 자리 원소끼리 따로 묶어라 => zip(*시퀀스데이터)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'numericalize_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s1YxX2xgbaY",
        "colab_type": "code",
        "outputId": "f5f5f860-88fe-453c-f4c5-71cd49f72746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#cnts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt2eAzswkP_b",
        "colab_type": "code",
        "outputId": "ade82271-4945-4dbc-de7b-fdcce6a8f56d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "torch.FloatTensor(cnts), torch.FloatTensor(cnts).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
              " torch.Size([16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-CxtZcMkQCo",
        "colab_type": "code",
        "outputId": "d324f1a9-9db3-4c24-ed32-9a208d315e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 사전의 인덳스는 정수니까, int계열\n",
        "# 향후 데이터가 풀로 들어가면 사전의 수가 int값의 범위를 넘어설수도 있다\n",
        "# long으로 처리해야 사전의 크기에 오류가 않생긴다> 전부다 표현할수 있다\n",
        "torch.LongTensor(tokens), torch.LongTensor(tokens).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]),\n",
              " torch.Size([16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2o6RGL-lFMo",
        "colab_type": "code",
        "outputId": "6a3f4129-ae5c-47b0-fa8e-5f3a267b9ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "# numericalize_data에서 문장 순서대로 데이터를 카운트 처리가 되므로\n",
        "for i, (sent, label) in enumerate(numericalize_data):\n",
        "  tokens, cnts = zip(*list(Counter(sent).items()))\n",
        "  # term_matrix에 빈도 계산한것을 대입 => (20, 155)\n",
        "  # 대입은 각 토큰에 위치에서 설정된다\n",
        "  term_matrix[ i , torch.LongTensor(tokens) ] = torch.FloatTensor( cnts )\n",
        "term_matrix[-1]\n",
        "# 행은 문장 => 샘플문장이 20개 term_matrix[ i(i번째 문장), ]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ZyyKwEp_ZY",
        "colab_type": "code",
        "outputId": "3b2c447b-25cd-420a-828c-bef4c777ea6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 문제점 => 0이 너무 많다\n",
        "term_matrix.eq(0).sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2882)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUEStJPqSBu",
        "colab_type": "text"
      },
      "source": [
        "## 단어주머지 문제점\n",
        "- 문장이 많으면 => 토큰수가 늘어나고 => 수치 백터의 차원이 커진다\n",
        "  - 힙스의 법칙 (heaps laws)\n",
        "  - 로그함수적으로 증가한다\n",
        "- 차원이 크면 -> 특성을 표현하기가 어렵다 -> 차원의 저주 -> 데이터가 희소해진다 -> 0이 많아지고 -> 선형 결합(행렬의곱)에서 활성화 함수를 통과한 값도 0이 되고, 이를 미분(역전파)값도0이되고 -> 학습 성과가 저하 -> 단어 백터의 밀집화(dense) 필요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB0nDGDYrSCh",
        "colab_type": "text"
      },
      "source": [
        "## padding\n",
        "\n",
        "- 문장의 길이는 각각 다르기 때문에 항상 같은 깉이로 만들어줘야 효율이 좋다\n",
        "- 문장의 최대 길이 기준으로 패딩 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXpxc1gssGOn",
        "colab_type": "code",
        "outputId": "e0030872-c2b3-4fea-dea3-166b87b32fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 샘플링 데이터 중에 가장 긴 문장의 길이 획득\n",
        "max_len = max( [ len(문장) for (문장, _) in numericalize_data  ] )\n",
        "max_len"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFTW3Kxcr5fQ",
        "colab_type": "code",
        "outputId": "ea8922ff-629c-46a5-a7b7-8e7fe17c09dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for s,_ in numericalize_data:\n",
        "  # 현재 문장의 길이가 최대 문장의 길이보다 작으면\n",
        "  c_len = len(s)# 현재 문장의 길이\n",
        "  if c_len < max_len:\n",
        "    # padding 표현을 추가한다 => <pad>\n",
        "    # [18, 19, 20, 21, 22, 19] => [18, 19, 20, 21, 22, 19, 1, 1, 1, 1, 1...... 1]\n",
        "    s += [vocab['<pad>']] * (max_len - c_len)\n",
        "\n",
        "print( numericalize_data[0] )\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 3, 14, 15, 16, 17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], '1')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT4pc55MtxiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65h1n0Wygvyi",
        "colab_type": "text"
      },
      "source": [
        "# 분절의 복원[별도단계]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2lehh_wgv0u",
        "colab_type": "text"
      },
      "source": [
        "# 파이토치의 토치텍스트 라이브러리[별도]\n",
        "\n",
        "- torchtext\n",
        "- 자연어처리,텍스트를 이용한 머신러닝/딥러닝 수행\n",
        "- 데이터 전처리 기능도 모두가지고 있다\n",
        "- 자연어 처리 데이터 종류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F92Bjl4YurHR",
        "colab_type": "text"
      },
      "source": [
        "|x|y|사용분야|\n",
        "|--|--|--|\n",
        "|코퍼스|클레스|텍스트 분류, 감성 분석|\n",
        "|코퍼스|--|언어 모델|\n",
        "|코퍼스|코퍼스|기계번역, 요약, QA(질의응답)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdBpdse0vdz_",
        "colab_type": "text"
      },
      "source": [
        "- 위에서 진행한 전처리 과정은 토치텍스트를 이용해서 구현/해결이 가능하다\n",
        "  - 필드 지정\n",
        "  - 데이터 셋 만들기 \n",
        "  - 단어장 생성\n",
        "  - 데이터 로더 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AZ0Kn3Bgfmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 자연어 처리를 하는데 (전처리 => 레이어설계 => 학습, 평가 => 결과물(번역,요약,분류,..))\n",
        "# 엔진 : 파이토치\n",
        "# 핵심모듈 : 토치텍스트\n",
        "# 형태소 분석기 : Mecab\n",
        "# 전처리 과정은 토치텍스트를 사용\n",
        "from torchtext.data import Field\n",
        "# 분절 처리를 하는 한글 형태소 분석기는 Mecab\n",
        "from konlpy.tag import Mecab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oHWQA6lwPrn",
        "colab_type": "text"
      },
      "source": [
        "## 필드 정의 (데이터들의 모양, 특성등등 정의)\n",
        "- 텐서로 표현할수 있는 텍스트 데이터 타입\n",
        "- 각 토큰을 숫자 인덱스 매핑해주는 단어장 객체도 포함\n",
        "- 토큰화 처리\n",
        "- 전처리, 후처리 다 지정할수 있다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgslRVTnyx8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Mecab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGyxYMxfyKSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ratings.txt의 데이터를 들여다 보면 \n",
        "# id, 리뷰내용, 긍정/부정\n",
        "# 리뷰 내용에 대한 필드\n",
        "'''\n",
        "sequential : 이 필드가 다루는 데이터가 시퀀스 데이터인가? => 문장 => 시퀀스 데이터(문자열이기도하고)\n",
        "use_vocab  : 사전을 사용할 것인가? True => 사전생성이 들어간다(형태소쪼개고, 인덱스로 사전구성)\n",
        "tokenize   : 사전을 사용한다면, 형태소 분석기는, 혹은 토큰 분석기는 어떤것이 담당할것인가?\n",
        "             기본값 : Spacy, 한국어 => tokenizer.morphs 를 사용\n",
        "lower       : 소문자로 처리해서 사용할것인가? (대문자를 소문자로 처리하는 것이 영어권는 일반적) \n",
        "batch_first : True:[ batch, h, w ] 이런 배치 False:[ h, w, batch ]\n",
        "'''\n",
        "TEXT  = Field( sequential = True, # 시퀀스 데이터 맞다\n",
        "               use_vocab  = True, # 사전을 사용하겟다\n",
        "               tokenize   = tokenizer.morphs, # 행태소 분석기로 이 함수를 사용하겟다\n",
        "               lower      = True, # 소문자로 처리하겟다\n",
        "               batch_first= True  # 배치사이즈는 앞으로 위치  \n",
        ")\n",
        "\n",
        "# 긍정/부정에 대한 필드\n",
        "# preprocessing : 전처리\n",
        "LABEL = Field( sequential = False, # 0 or 1\n",
        "               use_vocab  = False, # 사전 사용 안함\n",
        "               batch_first= True,\n",
        "               preprocessing = lambda x: int(x), # 정수 처리\n",
        "               is_target  = True   # 타겟 데이터 맞음 => y    \n",
        ")\n",
        "\n",
        "# 리뷰의 순서에 대한 필드(단, 사용하지는 않을것이다)\n",
        "# 데이터 로드시 받아 주긴 해야 하므로, 기본적인것만 설정\n",
        "ID    = Field( sequential = False,\n",
        "               use_vocab  = False,\n",
        "               is_target  = False\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89axUQYZwPuH",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 셋 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zvhCcM_zcvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import TabularDataset\n",
        "# TabularDataset\n",
        "# 데이터를 불러와서 정의한 필드를 기준으로 데이터를 처리해주는 객체"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jDgNoJJzl2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터를 주입하기 위해서 데이터를 준비한다\n",
        "dataset = TabularDataset( path='/content/ratings.txt', # 데이터 파일의 위치, 경로\n",
        "                          format='tsv',       # tab으로 분리되어 있다 tsv, csv, json등이 가능\n",
        "                          fields=[('id',ID),('text',TEXT),('label',LABEL)], # (필드를호출할 닉네임,필드)\n",
        "                          skip_header=True    # 데이터의 첫줄은 생략\n",
        "                         )\n",
        "# 데이터를 읽어와서 정해준 규칙에 의해 전처리, 형태소분리등 처리하여 필드에 담는다"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElDXvXMZwPw2",
        "colab_type": "text"
      },
      "source": [
        "## 단어장 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6W6I1n81fai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEXT 필드에서 단어장(사전)을 생성\n",
        "TEXT.build_vocab( dataset )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeokTV3S1xQx",
        "colab_type": "code",
        "outputId": "f616978c-16ed-4ebf-bc31-1667759607c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 단어장의 형태소 개수\n",
        "len(TEXT.vocab), type(TEXT.vocab), TEXT.vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62531, torchtext.vocab.Vocab, <torchtext.vocab.Vocab at 0x7f59f5bf31d0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d8Gt2Ji2Dz6",
        "colab_type": "code",
        "outputId": "4fb0e652-89c6-41b6-c3e3-05afdc31f6b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 단어장 인덱스 확인\n",
        "# 문자열(형태소) 넣어서 사전상의 인덱스를 구해라\n",
        "TEXT.vocab.stoi['<unk>'], TEXT.vocab.stoi['<pad>'], TEXT.vocab.stoi['사랑'], type(TEXT.vocab.stoi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1, 116, collections.defaultdict)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gekiqF0g2-cs",
        "colab_type": "code",
        "outputId": "76b53878-3ee4-45cc-ce3a-3edb276a2b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TEXT.vocab.stoi['완전']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnTg5hbYwPzp",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 로더 설정\n",
        "\n",
        "- 모델에게 배치 크기만큼 데이터를 전달하기 위한 수단"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o9XvwSZwMEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Iterator\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrDuR1fi3l46",
        "colab_type": "code",
        "outputId": "255cffe5-6b3a-4c84-d3c4-b2ddb6a8694e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# gpu 사용여부 설정\n",
        "device = 'cuda' if torch.cuda.is_available()  else 'cpu'\n",
        "device\n",
        "# 여기서 바로 사용되지는 않고, 다음 단계에서 연결된다"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6nxjcfM35_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataLoader = Iterator( dataset    = dataset,\n",
        "                       batch_size = 3,       # 설정값, 배치크기\n",
        "                       device     = device   # gpu or cpu    \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob6asTH14mH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습 ~ \n",
        "for batch in dataLoader:\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHqc3eUc4uCT",
        "colab_type": "code",
        "outputId": "99bc6920-75f5-4571-ad60-46fda39ff9a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# 데이터 확인\n",
        "batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 3]\n",
              "\t[.id]:[torch.LongTensor of size 3]\n",
              "\t[.text]:[torch.LongTensor of size 3x42]\n",
              "\t[.label]:[torch.LongTensor of size 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IomOOV-L41La",
        "colab_type": "code",
        "outputId": "68e6d0ad-d993-4b5f-8aab-38d76d97693e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "batch.text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   74,    74,    67,   555,    19,  1386,    19,  4464,   276,   775,\n",
              "           112,   157,    67,    10,   231,    40,    15,    58,   309,     5,\n",
              "           141,   185,    19,   190,  1627,   259,  2646,  2219,   170,    24,\n",
              "            42,    67,     5,   103,     4,   259,     3,   114,    28,    52,\n",
              "           219, 18976],\n",
              "        [ 4308,     3,   617,  8987,  3367,     3,   617,    46,    12,    72,\n",
              "            59,  2196,    14,     8,     7,    24,   148,    72,     3,    20,\n",
              "            29,    79,   151,    12,   147,    20,     3,    15,    55,   155,\n",
              "            27,    78,    71,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1],\n",
              "        [   41,   119,     6,    77,   162,    14,    34,   435,   515,  1214,\n",
              "             2,    17,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfBDsnNb46mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
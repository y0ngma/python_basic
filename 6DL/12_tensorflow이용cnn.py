# -*- coding: utf-8 -*-
"""12.Tensorflow이용CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vZZIhR6KJZVZGxC7KE3yo8ki08dGz8bW

# 0.연구목표

- 데이터
  - MNIST 손글씨 이미지 데이터사용
- 목적
  - 손글씨 이미지 분류
- 엔진
  - 텐서플로우
- 모델링
  - 딥러닝 신경망중 CNN 모델사용
- 평가
  - 예측 모델을 생성하고, 정확도를 이용하여 평가를 수행(분류)

# 1.데이터확보
- 출처
  - 텐서플로우의 케라스에서 제공하는 MNIST 데이터
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
# %tensorflow_version 1.x
from tensorflow.examples.tutorials.mnist import input_data

### 케라스에서 제공하는 데이터는 쉽게 로드가 되지만 원본데이터여서 가공필요
# mnist = tf.keras.datasets.mnist.load_data(path='mnist.npz')
# print(type(mnist), len(mnist), len(mnist[0]), len(mnist[1]))#<class 'tuple'> 2 2 2
# print(len(mnist[0][0]),len(mnist[1][0]),len(mnist[0][1]),len(mnist[1][1]))#(60000, 10000, 60000, 10000)
# print(mnist[0][0].shape, mnist[1][0].shape, mnist[0][1].shape, mnist[1][1].shape) #((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))
# print(mnist[0][0][:5], mnist[1][0][:5])

# 가공된 데이터를 받아서 처리하자
mnist = input_data.read_data_sets('/content/drive/My Drive/data/mnist', one_hot=True)
print(type(mnist))#<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>
print('========훈련데이터=======', mnist.train.images.shape, mnist.train.labels.shape)
print('========테스트데이터=====', mnist.test.images.shape, mnist.test.labels.shape)

"""# 2.데이터준비"""

# 이미지 1개당 feature의 크기
pixels = mnist.train.images.shape[1] #784
# 레이블  feature의 크기
nums = mnist.train.labels.shape[1] #10
# 이미지 1개당 가로 혹은 세로 크기 (정사각형이므로 동일)
pixel_wh = int( np.sqrt(pixels) ) #sqrt(784^2)

# 정답 레이블에서 원래값 0~9까지 찾아내는 방법
# array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]) => 7 
# 나중에 정답을 출력
# print(mnist.train.labels[0].argmax(), np.where(mnist.train.labels[0])[0][0]) #(7,7)

"""> Argmax() 란?

매개변수                         | 전달인자
:---                             | :---
Parameter                        | Argument
Variable                         | Value
함수의 정의단계에서 나열된 변수  | 함수의 호출단계서 전달할 실제값
f(x) = x^2 에서 ' x '            | f(3)  =  3^2 에서 ' 3 '
비고                             | argmax()는 인자의 값중 가장 큰값을 출력

> numpy.where(condition, x, y) 
  - condition : array_like, bool. Where True, yield x, otherwise yield y
- a = np.arange(10)
  - array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
- np.where(a < 5, [a, 10*a])
  - array([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])

# 3.데이터분석(생략)

# 4.모델구축

## 1.모델, 레이어설계
"""

'''
layers
  L 입력층
  L 중간층
    L 합성곱층 (Convolution Layer)
    L 풀링층 (Pooling Layer)
    L ... <- 이런 2개의 조합이 여러번 반복(만들기 나름)
    L 합성곱층 (Convolution Layer)
    L 풀링층 (Pooling Layer)
    L 전결합층
    L 드롭아웃층(생략가능, 위의 레이서에서 중간중간 등장 가능)
  L 출력층
'''

"""## 2.데이터플로우 그래프 구축

### 입력층
- x
- 손글씨 이미지가 데이터로 주입된다
- 플레이스홀더로 구성
- shape : [None, 784=pixels]
"""

# print(mnist.train.images.dtype) #dtype('float32')
x = tf.placeholder( tf.float32, [None, pixels], name='x' )#<tf.Tensor 'x_1:0' shape=(?, 784) dtype=float32>

"""### 합성곱층1

- 외부에서 이전단계 레이어에서 데이터가 흘러들어온다
- 데이터주입? (아님)
- 구성원
  - W:공용파라미터, Weight => 필터, 커널
  - b:bias
  - s:stride
  - p:padding
"""

## 가중치필터W를 만드는 함수
  # shape = W의 shape ( 고정값아님 )
  # name : W의 이름 ( 텐서보드에서 해당 텐서들을 구분용 )
def makeWeightVariable( shape, name ):
  # 변수 초기값 : 절단평균분포를 통해서 난수값으로 초기화
  # 0.1은 설정값 : 가중치필터의 값은 일단 난수로 설정
  init_W = tf.truncated_normal( shape, stddev=0.1 ) #잘린,짧아진
  # W를 생성
  W = tf.Variable( init_W, name='W_'+name )
  # 가중치필터(커널) W를 리턴
  return W

## 편향을 만드는 함수
def makeBiasVariable( shape, name ):
  # 상수로 값을 하나 고정하여 적용
  init_b = tf.constant( 0.1, shape=[shape] )
  b      = tf.Variable( init_b, name='b_'+name )
  return b
  
## 합성곱층을 만드는 함수
def makeConv2d( x, W, name ):
  # 1칸씩 stride 하겠다
  conv2d = tf.nn.conv2d( x, W, strides=[1,1,1,1], padding='SAME', name='conv_'+name )
  # conv2d = tf.nn.conv2d( x, W, strides=[1,1,1,1], padding='VALID', name='conv_'+name )
  return conv2d
  '''
  tf.nn.conv2d(
    input   : [ batch,      1회훈련시 오는 데이터총수
                in_height,  이미지의 세로크기
                in_width,   이미지의 가로크기
                in_channels 입력이미지의 채널수, 1 또는 3
              ]
    filters : [ filter_height, 필터세로 ex:3
                filter_width,  필터가로 ex:3
                in_channels,   입력채널수 1
                out_channels   출력채널수 :알아서설정
              ]
    strides : An int or list of ints that has length 1, 2 or 4. 
              정수값, 정수값들의 리스트로 구성
              구성값은 1, 2 또는 4
              [ batch : batch와 depth는          :1
                w     : w와 h는 통상 같은값 부여 :1
                h     : 1
                depth : 1
              ]
    padding : Either the string "SAME" or "VALID" 
              특성맵을 보정한다. 자르다보면 외곽선부분이 부족하여 필터작업이 안될수 있으므로
              이를 보정. 통상적으로는 0으로 설정
              "SAME" : 같은크기로 보정한다 또는 "VALID" : 유효한부분만 보정한다
  )
  '''

"""#### 합성곱층 연결

- x, W, b 등을 연결하여 활성화맵으로 출력하게끔 데이터플로우 그래프를 정의
- 텐서보드상에서 이 합성곱을 하나의 노드로 표현하기 위해 name_scope하나의 이름이 미치는 공간, 관계 및 형태를 규정
"""

# 합성곱층 1 생성. 입력대비 출력까지의 모든 관계(그래프)를 표현
with tf.name_scope('conv1') as scope:
  ## W
    # 출력채널수를 32라고 지정하면, 이미지한장을 넣어서 결과물이 32개가 나온다(증폭)
    # 3번째인자인 입력채널수 1을 제외하고 나머지는 실험치이다
  W = makeWeightVariable( [5,5,1,32], 'conv1' )
  ## b
  b = makeBiasVariable( 32, 'conv1' ) # xW의 결과물의 수
  ## x : 입력층(None,784)>(batch,h,w,channel)
    # 모양을 tf.nn.conv2d에서 원하는 모양으로 맞춰준다
    # x.shape=(2, 3)-> reshape(x, [3,-1]) -> (3,2) # 전체사이즈유지용.-1은 단독사용시 1차원화, 
  x_imgs = tf.reshape( x, (-1, pixel_wh, pixel_wh, 1) ) #마지막 1은 채널수(컬러는 3)
  ## h_conv1 : Activation Map이다
    # <tf.Tensor 'conv1_8/Relu:0' shape=(?, 28, 28, 32) dtype=float32>
  h_conv1 = tf.nn.relu( makeConv2d(x_imgs, W, name='conv1' )+b ) # Feature Map : conv2d

"""#### 이미지의 수량
- 합성곱층을 통과하면 1개의 이미지가 32개가 된다
- 이유는 가중치 필터에서 출력채널수를 32개로 하였으므로 수량이 증가함

#### 이미지의 크기
- 현재 결과물은 (?,28,28,32)이다
- 이미지의 세로가로크기가 원본과 동일
  - padding='SAME' 때문
- 'VALID'로 구현하면 (?,24,24,32)로 감소

### 풀링층1

- 특성(특성맵, 활성화맵)의 강화한다
- 최대/평균/최소 풀링제공, 구현가능
- 샘플링 표현으로도 묘사
- 커널W이 없으므로 stride, padding 없다. 단,
  - 풀링의 수행은 shape이 작아지는 결과
  - 내부적으로 풀링연산을 수행키위해 얼만큼 이동하여 값을 추출해야 하는지 보폭의 양이 존재해야함
  - 커널이 없더라도, 연산에 필요한 stride값은 필요.
  - 따라서 padding 또한 이를 위해 존재필요
- 입력 : h_conv1 ( ?, 28, 28, 32 )
"""

# 풀링함수 => 최대풀링
def makeMaxPooling( x ):
  # 결과물의 포멧은 : Number Height Width Channel이다(현상황의 default값 NHWC)
  return tf.nn.max_pool( x, ksize=[1,2,2,1], 
                          strides=[1,2,2,1], padding="SAME" )
'''
input   : 4-D tensor, 이전단계의 산출물, [batch, h, w, channels]
ksize   : [batch, h, w, 출력채널수] => 기본 커널에 대비해서 W는 없다
          이 중에 최대, 평균, 최소값등 선택 
          => 대상:데이터1개 => 각 차원의 크기지정
          => 최대풀링값이 1개 나옴 => 4번째인자 1의 의미
          => 가운데 2,2 만 신경쓰면 됨
strides : [batch, w, h, depth], batch=depth, w orh, : 1, 2, 4 등 사용
padding : A string, either 'VALID' or 'SAME'
'''
# 풀링층1 생성
with tf.name_scope('pool1') as scope:
  h_pool1 = makeMaxPooling( h_conv1 )
# print(h_pool1.shape, h_pool1) # shape=(?, 14, 14, 32) : 크기가 반, 출력채널수는 유지

"""### 합성곱층2

#### 실습 : output 32->64
- 입력 : h_pool1
- 출력 : h_conv2 => (?,14,14,64)
"""

name_conv2 = 'conv2'
with tf.name_scope(name_conv2) as scope:
  #[ 커널w, 커널h, 입력:이전단계출력, 출력 ]
  W_conv2 = makeWeightVariable( [5,5,32,64], name_conv2 ) 
  b_conv2 = makeBiasVariable( 64, name_conv2 )
  # x_conv2 = tf.reshape( x, (-1, pixel_wh, pixel_wh, 1) ) # 이전단계에서 완료했으므로 제거
  h_conv2 = tf.nn.relu( makeConv2d(h_pool1, W_conv2, name_conv2)+b_conv2 )
h_conv2

"""### 풀링층2

#### 실습 : (14,14)->(7,7)
- 최대 풀링을 사용하여 아래와 같이 입력/출력 shape을 처리하시오
- 입력 : h_conv2 => ( ?, 14,14,64 )
- 출력 : h_pool2 => ( ?, 7, 7, 64 )
"""

# shape을 다양하게 처리하려면 : ksize, strides 등을 인자로 입력받고 처리하게 함수 수정
with tf.name_scope('pool2') as scope:
  h_pool2 = makeMaxPooling( h_conv2 )
# print(h_pool2.shape, h_pool2) # shape=(?, 14, 14, 32) : 크기가 반, 출력채널수는 유지

"""### 전결합층

- 출력층으로가는 직전 단계이다
- 전결합층에 도착한 입력데이터 h_pool2는   Tensor => 행렬이다 => 출력층에 보내기 위해 Flatten을 수행한다 => h, w, channel로 구성된 데이터를 한개의 row로 펴준다

#### 실습 : (?,7,7,63)->(?,1024)
- 현재 이미지 1개는 7x7x64=3136 의 정보로 표현되고 있다
  - 1024로 flatten 한다(정보를 압축해서 펴준다)
  - 이미지사이즈는 풀링을 2회 거치면서 조정되었다 (28/2)/2 => 7
  - 1024설정값(통상 2^n)
  - (-1, 3136)행렬의 곱(3136, 1024) = (-1, 1024)
  - input : h_pool2 => (?, 7,7,64)
    - input Demension : 4D -> 2D reshape
  - output: h_fc    => (?, 1024)
"""

with tf.name_scope('fully_connected') as scope:
  num = 7 * 7 * 64 # h_pool2 정보에서 뽑아서 계산하시오(변경)
  # 가중치필터는 합성곱을 위한것이 아니라 행렬곱을 위해 모양을 맞췄고
  # 입력값에 가중치를 곱한다는 퍼셉트론에 맞춰서 계산
  W_flat = makeWeightVariable( [num, 1024], 'fully_connected')
  b_flat = makeBiasVariable( 1024, 'fully_connected' )
  h_pool2_flat = tf.reshape( h_pool2, [-1, num]) # (?, 3136)으로 ?를 알아서 결정함
  h_fc = tf.nn.relu( tf.matmul( h_pool2_flat, W_flat ) + b_flat ) #matmul행렬곱->행렬모양결정
# print(h_fc) #<tf.Tensor 'fully_connected_6/Relu:0' shape=(?, 1024) dtype=float32>

"""### 드롭아웃층

- 과적합, 과잉적합, over_fit 방지
- 특정데이터에 길들어지는 편향적형태를 가진것을 방지
- 뉴런(특정신경망 일부) 죽여서 학습을 못하게 처리(랜덤)
"""

with tf.name_scope('dropout') as scope:
  # keep_prob값은 외부에서 받아서(데이터주입) 신경망을 죽이는 비율 조정하겠다
  keep_prob = tf.placeholder( tf.float32 )
  h_fc_drop = tf.nn.dropout( h_fc, rate=1-keep_prob )
print(h_fc_drop) #<tf.Tensor 'dropout_5/dropout/mul_1:0' shape=(?, 1024) dtype=float32>

"""### 출력층

- 이전단계의 산출물 h_fc_drop : (?, 1024)을 최종레이블(분류)의 shape으로 맞춘다
  - 수렴한다
- 최종레이블(0-9)
- 최종 출력에 데이터 수렴시 활성화 함수 사용
  - 현레이블의 클레스수 : 10개
  - 다항이므로 softmax()를 사용이 적절
  - 소프트맥스를 통과한 전체데이터를 합산하면 1이다. (값은 0-1사이)

#### 실습 : (?, 1024)->(?, 10)
"""

with tf.name_scope('output') as scope:
  # 1024 : h_fc_drop으로부터 정보를 획득해서 세팅
  # 10   : 데이터준비단계에서 정의한 레이블수10을 가져옴
  W_output = makeWeightVariable( [1024,nums], 'output')
  b_output = makeBiasVariable(nums, 'output')
  y_conv   = tf.nn.softmax( tf.matmul( h_fc_drop, W_output ) + b_output )# 행렬곱
# print(y_conv) #<tf.Tensor 'output_8/Softmax:0' shape=(?, 10) dtype=float32>

# 정답 => 데이터를 주입받는다
y_ = tf.placeholder( tf.float32, shape=(None, nums), name='y_' )
# print(y_) #<tf.Tensor 'y__1:0' shape=(?, 10) dtype=float32>

"""### 학습 플로우 작성

#### 크로스엔트로피, 손실값

- 비용을 줄인다. 손실값을 줄인다
- 이런 지표를 원하는 결과에서 얼마나 떨어져 있는지를 보여준
"""

with tf.name_scope( 'loss' ) as scope:
  cross_entropy = -tf.reduce_sum( y_*tf.log(y_conv) )
# print(cross_entropy) #<tf.Tensor 'loss_1/Neg:0' shape=() dtype=float32>

"""#### 경사하강법, 최적화

- 손실값을 줄이는 방향으로 점진적으로 값을 조정하면서 최적화를 수행한다.
- 종류 : 종류별 정리한다
  - SGD
  - Adam
"""

with tf.name_scope('sgd') as scope:
  optimizer = tf.train.AdamOptimizer()
  train     = optimizer.minimize(cross_entropy)
# print(train)  #<tf.Operation 'sgd_2/Adam' type=NoOp>

"""### 예측/평가 플로우 작성"""

with tf.name_scope('predict') as scope:
  predict  = tf.equal( tf.arg_max(y_conv, 1), tf.arg_max(y_, 1) ) 
  #equal() : 두인자 같은것으로 이루어진 array 리턴
  accuracy = tf.reduce_mean( tf.cast(predict, tf.float32) )

"""## 3.실행 ( 학습, 예측, 평가 )"""

# 주입할 데이터의 모양을 세팅해주는 함수를 구성
def makeFeedDictParam(imgDatas, labels, prob):
  return { x:imgDatas, y_:labels, keep_prob:prob }

# 학습횟수
TRAIN_COUNTS  = 3000 #실험치
ONE_TRAIN_AMT = 50   #한번훈련시 사용량
VERBOSE_TERM  = 100  #100번째 훈련마다 로그출력
with tf.Session() as sess:
  ## 1. 텐서플로우의 전역변수들 초기화 한다
  sess.run( tf.global_variables_initializer() )
  ## 2. 데이터준비
    # 2-1. 훈련용 데이터를 주입할 수 있게 준비
      # 2-1-1. 종류 : {x:, keep_prob:, y_:} <- 3-2에서 준비
    # 2-2. 테스트용 데이터를 주입할 수 있게 준비
      # 2-2-1. 종류 : {x, keep_prob, y_}
      # rate = 1-keep_prob = 0이 되어 dropout 방지. 검증용은 과적합과 무관
  test_feedDict = makeFeedDictParam( mnist.test.images, mnist.test.labels, prob=1 )
  ## 3. 반복학습
  for step in range(TRAIN_COUNTS):
  # 데이터를 나눠서 학습시키겠다(학습횟수 증가)
    # 3-1. 1회 훈련에 사용하는 데이터 획득(총량, 횟수고려)
    batch = mnist.train.next_batch(ONE_TRAIN_AMT)
    # 3-2. 훈련용데이터를 생성
    # 데이터50셋, 과적합방지용 드롭아웃 비율 0.5
    train_fdp = makeFeedDictParam( batch[0], batch[1], 0.5 )
    # 3-3. 훈련용데이터를 주입하여 훈련
    # 작업2개를 []로 묶어구동:2개 리턴. 데이터주입은 train이라는 Operation상에서 사용되고
    # 그후의 결과로 세팅된 cross_entropy는 단순히 값을 확인하기 위해서 sess.run()의 작업항목으로 묶어두었다
    # 수행결과 2개중 train의 수행결과는 불필요 => _로 받고, 손실값만 변수로 받았다
    _, loss = sess.run( [train, cross_entropy], feed_dict=train_fdp )
    # 3-4. 특정횟수마다 로그 출력 <- test_feedDict 사용
    if step % VERBOSE_TERM == 0:
      # 3-4-1. 테스트데이터를 현재까지 훈련된 신경망에 주입후 정확도측정
      acc = sess.run( accuracy, feed_dict = test_feedDict )
      # 3-4-2. 정확도, 손실값 출력해야할듯?
      print( 's=%4s, a=%4s, l=%10s' % (step, acc, loss))
  ## 4. 최종결과를 출력 <- test_feedDict 사용
  acc = sess.run( accuracy, feed_dict = test_feedDict )
  print('='*50)
  print('최종출력값')
  print( 's=%4s, a=%4s, l=%10s' % (step, acc, loss))
  print('='*50)

"""# 5.시스템통합(생략)"""
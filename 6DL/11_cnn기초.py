# -*- coding: utf-8 -*-
"""11.CNN기초.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RbFz513jZuq-NsBJc3zO3z1p7YpuNIQf

# ILSVRC

- 이미지인식, 분류 대회(종료)
- history
  - AlexNet(2012)
    - CNN 기반 딥러닝 알고리즘
    - 딥러닝이 이미지분류대회에 처음으로 등장
    - 이후로 거의 다 딥러닝으로 패러다임이 변경
    - 이미지 인식은 곧 CNN이다 라는 정설이 마련
  - GoogleLeNet(inception v2, 2014)
  - ResNet(2015, 인식오류 5%)
    - resNet-152(층)
  - GoogleLeNet -v4(2016)
  - seNet(2017, 2.3%)

## CNN

- 합성곱 신경망
  - Cnvolutional Neural Network
- History
  - 1989년 Lecun 논문에서 발표
  - 2003년 Behnke 논문에서 일반화, Simard논문에서 단순화

- 분야
  - 이미지인식(분류)
  - 음성인식, 자연어처리에도 사용
  - 이미지 분야읻너 다른쪽이던 간에 특정기술로만 해야한다는 이제 사라지고 언제든지 하이브리드 섞어 사용 등 가능

# 구조

##layers
- 입력층
  
- 중간층
  1. 합성곱층 (Convolution Layer)
  1. 풀링층 (Pooling Layer)
  1. ...
  1. (이런조합이 한쌍으로 여러번 반복)
  1. ...
  1. 합성곱층
  1. 풀링층
  
  1. 전결합층
  1. 드롭아웃층(생략가능, 중도에 출현가능)
- 출력층

# 원리 및 특징

- 이미지의 공간 정보를 유지
- 인접이미지와의 특징을 효과적으로 인식하고 강조한다
  - 한개의 이미지내에서 28x28 이었다
  - 3x3칸으로 이동시키면 한개의 이미지내에서 인접공간의 특징을 이해할 수 있다

## 1.이미지의 특징 추출 및 강화
- 합성곱층 (추출)
  - 커널(필터) 행렬(2D)을 이동(스트라이드)
  - 이미지의 특징을 추출한다
  - 이미지의 shape이 줄어든다
- 풀링층 (강화)
  - 합성곱층으로 나온 결과물(특징추출)을 강화
  - 입력으로 들어온(합성곱층을 통과해서 나온 결과물) 이미지의 shape이 유지된다

## 2.이미지 분류

- 출력층

# Convolution Layer
- 합성곱층
---
- 경우에 따라서는 풀링층까지 포함해서 표현
- 이미지의 특징을 추출한다

- 특징의 목적 : 커널(필터개발)
  - 이미지의 어떤 특징을 추출
  - 예
    - 평활화 : 
      - 이미지의 명암분포를 균일하게 처리
    - 윤곽선검출 :
      - 이미지상에 외곽선(윤곽)을 추출

## 합성곱의 구성원들

### 1.Input = x
- MNIST라면 손글씨 이미지 1개
- 이미지는( 흑백 1channel, 칼라 3channel(RGB) )으로 구성된다

### 2.Kernel( Filter ) = W
- 커널(필터) => 3x3 (2D) 행렬
- 필터 : 이미지의 특징을 찾아내기위한 공용 파라미터
- 커널을 구성하는값(파라미터)
- 이렇게 파라미터에 값이 설정되어 있다면(랜덤 또는 개발해서 적용) => W
- 가중치 필터W
"""

from IPython.display import Image
Image('/content/drive/My Drive/data/DL_data_ref/cnn1.png', width=400)
# 입력이미지 x에 필터 W를 통과시켜서 특징이 추출된 Feature Map 산출과정
# 계산 : 단일 곱셈-누산(FMA, Fused Multiply-Add)
# 아래 그림의 Input은 전체 데이터를 보여준것이 아니다
# Feature Map이 3x3으로 보이는것은 안중요. 4가 뽑인과정만 중요

"""### 3.Padding = p
- 자르다보면 크기가 안맞는 경우 발생
- 크기 보정으로 스트라이드 가능케 처리
- 외곽선에서 나올 확률이 크다
- 합성곱층을 통과시키면 크기가 자꾸 작아지므로 발생가능
- 통상 0으로 채운다

- feature map은 입력데이터보다 통상 장은 shape
- 여러층을 통과 ->어느순간 크기 온전해지지않음
- 외곽선을 특정값(0)으로 채워서 크기보정하는 과정
- 바깥쪽은 0으로, 실제값은 안쪽으로 몰리게된다(이런 특징을 가짐)
-
"""

Image('/content/drive/My Drive/data/DL_data_ref/cnn4.jpg', width=200)

Image('/content/drive/My Drive/data/DL_data_ref/cnn5.png', width=200)

"""### 4.Stride = s
- stride
- n칸 이동단위에 따라 특징추출량도 달라진다
- 필터를 이동시키는 크기
- 필터가 순회하는 간격
- 커널이 움직이는 크기
"""

Image('/content/drive/My Drive/data/DL_data_ref/cnn2.png', width=400)
# 커널의 스트라이드 과정을 보여준다
# x축으로 s값 만큼 이동하고, 끝에 도달하면, y축으로 s값 만큼 이동

"""- 필터, 커널 등을 사용하는 이유
  - 입력 이미지가 28x28
  - 머신러닝에서 feature를 생각해보면 784개
  - 이것을 딥러닝에 그냥 적용해본다면 784개의 가중치W 필요
    - 연산량 압박
  - 필터3x3이면 가중치값은 9개만 필요
  - 계산량 감소, 학습속도향상
  - 비용절약, 효율성, 생산성 향상
"""

Image('/content/drive/My Drive/data/DL_data_ref/dp1.png', width=600)
# (데이터수, 채널수, 가로, 새로)

Image('/content/drive/My Drive/data/DL_data_ref/cnn3.png', width=400)
# 이미지가 칼라이미지여서 3channel인경우

"""### 5.Bias = b
- 편향(바이어스)

## 합성곱층의 결과물

### 1.Feature Map
- 합성곱층의 계산을 통해서 만들어진 행렬
- 피쳐맵의 층을 깊게 만들기 위해 비선형으로 처리
  - 활성화함수(sigmoid, relu..)에 통과
  - Activation Map이라고 함

### 2.Activation Map
- Feature Map이 활성화 함수를 통과해서 나온 결과물
- 최종결과물
  - Feature Map
  - Activation Map

# Pooling
- 이미지의 특징을 강화한다
- 합성곱층의 출력인 특징맵(활성화맵) : C
- C를 축소 혹은 유지하는 층(shape의 크기, 차원은 유지)
- 축소일때:
  - 특징을 유지한채로 축소를 하므오 위치면경이나 결과에 변화는 방지됨
  - 예:
    - 직선이 미세하게 흐트러지지만 직선이라는 인식은 유지
  - 종류
    - 최대풀링(단위에서 최대값을 취함)
    - 최소풀링
    - 평균풀링
  - 특징
    - 공용 파라미터가 없다
    - 해당 풀링을 통과하면 통상 크기감소
    - W 필터가 없다
      - 따라서 stride, padding 없다. 단,
      - 풀링의 수행은 shape이 작아지는 결과로써
      - 내부적으로 풀링연산을 수행키위해 얼만큼 이동하여 값을 추출해야 하는지 보폭의 양이 존재해야함
      - 커널이 없더라도, 연산에 필요한 stride값은 필요.
      - 따라서 padding 또한 이를 위해 존재필요
    - 채
"""

Image('/content/drive/My Drive/data/DL_data_ref/cnn6.jpg', width=300)

Image('/content/drive/My Drive/data/DL_data_ref/cnn7.jpg', width=500)

"""# Fully connected Layer
- 전결합층에 도착한 특징맵(Feature Map)을 1차원으로 전개시킨다
- Flatten(n차 행렬을 1차행렬로 펴준다)
"""

Image('/content/drive/My Drive/data/DL_data_ref/cnn8.png', width=600)

"""# Dropout Layer
- 끝이나 중간에 나옴
- 과적합을 방지하기 위한 조치
- 원리
  - 인공신경망이 학습중일때, 랜덤하게 "신경꺼" : 신경망을 꺼서 학습을 방해한다
  - 학습이 학습용 데이터에 치우친다
"""

Image('/content/drive/My Drive/data/DL_data_ref/cnn9.png', width=600)

Image('/content/drive/My Drive/data/DL_data_ref/cnn10.png', width=600)


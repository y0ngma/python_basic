No|이름|입력|출력|사용처|특징
--|--|--|--|--|--
1|Sigmoid|실수|0 ~ 1|-|초기에 많이 사용<br>- 출력값이 1이나 -에가까워 지면 기울기 0<br>- 가중치 미반영 : 학습이 안되어 죽은모델
2|Tanh|실수|0 ~ 1|-|sigmoid개선<br>문제발생확률을 낮추기 위해 0 이하를 넓혀서<br>변동폭을 sigmoid보다 2배로 확장<br>- 문제점은 sigmoid와 동일. <br> -1, 1에 가까워지면 죽은뉴런(단, 빈도는 감소)<br>
3|ReLu|실수|0 or y=x|-|근 몇년간 가장 많이 사용<br>Leaky ReLu, PReLu등 많은 계열을 가짐 <br>장점<br>- 최적화(경사하강법, ..:손실값을 줄이기 위해 W를 조정<br>- 도구가 더욱 빠르게 가중치를 찾도록 돕는다<br>- 연산비용 저렴<br> 단점<br>- 역전파진행시, 기울기가 큰경우 무응단 문제(죽은뉴런)<br>- AlexNet(이미지인식대회(2012)에 CNN 최초 적용<br>- tanh보다 6배 성능향<br>
4|Leaky ReLu|실수|0.01 or y=x|-|음수대를 0.01을 기울기로 0 가깝게 구성<br>경우에 따라 ReLu보다 좋은 성능을 내기도 한다<br>
5|PReLu|음수일때|기울기 좀더 조절|-|알파값에 의해 음수대값이 조정<br>Leaky ReLu에 비해 유연성 제공<br>
6|ELU|음수일때|수평이동후 근사처리|-|미분을 이용한 음수대 계산<br>미분연산비용 발생
7|Maxout|실수|실수|-|ReLu, Leaky ReLu의 일반화모델<br>ReLu 계열 모든장점 취합 및 죽은 뉴런문제도 해결<br>파라미터 2배로 인한 연산량 증가 문제 존재
8|Softmax|실수|실수|분류출력층 직전|총량에 개별데이터의 비율, 결과의 총합=1<br>다항레이블일때 적절

alt shift f
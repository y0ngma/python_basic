# -*- coding: utf-8 -*-
"""21.간단한 강화학습게임구현.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zpA-aB2W7M769LiCfgk5y9DXfkCeiEQ9

# 전개방향
1. 개별알고리즘
1. 탐색처리
1. 그래픽스
1. 게임AI 구현
1. 게임구현완료
- AI
  - 약 AI(현재~)
    - 하나의 업무만 잘한다
  - 강 AI(2040 도래)
    - 인지능력이 사람의 수준으로 도달하는
    - 여러개를 잘하는 AI
  - 초 AI (2060)
    - 인간의 인지능력을 초월한 수준
    - 인간이 AI를 이해하지 못하는 단계

# 다중슬롯머신

- 알고리즘
  - UCB1
  - ε-greedy(epsilon-greedy)

## 다중슬롯머신(slot machine) 구현
- 스타일을 익힌다
- 강화학습의 많은 요소들은 생략되어 있는 간단한 구조
- 전체적인 절차(플로우, 흐름)을 이해할 수 잇다

## 게임 *환경*

- 슬롯머신의 팔(arm)은 여러개 존재할 수 있다
- 각 팔을 선택할 때 보상이 나올 확률은 저해졋다

팔|확률
--|--
1|10%
2|20%
3|30%

- 게임을 시작할 때 각 팔에 대한 확률은 모른다
- 보상값은 1 아니면 0.0
  - 보상간의 차이는 없다
  - 받는가 못받는가만 신경쓰면 된다.

## 게임 *목적*

- 제한된 횟수에서 최대의 보상을 받는다
- 어떤 순서로 arm을 선택해야 하는가
- 어떤 팔을 당겨야 가장 많은 돈을 벌것인가?
  - 단, 보상값들이 달라졌을때 해당

## 에이전트 *행동*

1. 여러 슬롯중 한개를 선택
1. 1개를 당기면
1. 행동
1. 한판종료
1. 한개의 에피소드 종료

강화 학습 요소|다중 슬롯머신상의 내용
:--|:--
에이전트|레버(arm)내리는 자
환경|다중슬롯머신
목적|제한된 횟수내에서 많은 보상을 획득
행동|여러개의 arm중 하나를 선택
에피소드|1회의 행동
상태|없음
보상|보상은 모두 동일, 1.0(설정)
수익|보상의 총합(즉시보상 + 지연보상)
학습방법|UCB1, ε-greedy(epsilon-greedy)
파라미터변경주기<br>(정책결정맥락)|1회 행동 종료후
정책|슬롯머신의 Arm을 제한된 횟수내에 선택할 수 있다(10,100,1000회)<br><br>탐색 : Arm별로 보상을 받을 확률을 모른다<br><br>- 최초전략은 정보수집을 위해 램덤하게 선택<br><br>이용 : 정보수집후 이를 바탕으로 가장높은보상이 예상되는 Arm선택<br><br>탐색과 이용에 대한 트레이드 오프 문제가 발생 > 균형(조화)필요<br><br>정보수집을 위해 탐색만 수행하면 모든팔에 대한 보상정보를 알게 되지만<br><br>제한된 횟수라는 환경안에서 보상이 가장높은팔(확률이 가장높은팔)만<br><br>선택하는 것보다 수익이 적게된다<br><br>반대시점, 현시점에서 가장 높은 팔만 선택한다면<br><br>더 많은 보상을 지급하는 팔을 선택하지 못하는 경우도 발생한다

### 정책
- 슬롯머신의 Arm을 제한된 횟수내에 선택할 수 있다(10회, 100회, 1000회)<br>
- 탐색 : Arm별로 보상을 받을 확률을 모른다<br>
  - 최초전략은 정보수집을 위해 램덤하게 선택<br>
- 이용 : 정보수집후 이를 바탕으로 가장높은보상이 예상되는 Arm선택<br>
- 탐색과 이용에 대한 트레이드 오프 문제가 발생 > 균형(조화)필요<br>
- 정보수집을 위해 탐색만 수행하면 모든팔에 대한 보상정보를 알게 되지만 제한된 횟수라는 환경안에서 보상이 가장높은팔(확률이 가장높은팔)만 선택하는 것보다 수익이 적게 된다<br>
- 반대시점, 현시점에서 가장 높은 팔만 선택한다면, 더 많은 보상을 지급하는 팔을 선택하지 못하는 경우도 발생한다
"""

# 구조
'''
- SlotMachineGame
  L SlotArm Class
  L GameEngine Class
    L EpsilonGreedyEngine Class
    L UCB1Engine Class
  L GameSimulator Class or function
'''

# Commented out IPython magic to ensure Python compatibility.
# 필요 패키지
import random, math
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
# %matplotlib inline

"""## SlotArm Class"""

# Slot machine Arm class
class SlotArm():
  # 초기화 : 확률값초기화(외부에서 주입)
  # p:객체를 만들떄 주어지는 확률갓
  def __init__(self, p):
    # 맴버변수p가 만들어지고, 외부에서 입력된 p로 인해 값이 초기화
    # self.p는 전역변수, p는 지역변수
    self.p = p
  # 팔선택시 보상지급처리 : 세팅된 확률보다 랜덤값이 적으면=1.0 or 아니면=0.0
  def draw(self):
    # 랜덤값 : 0 ~ 1
    if self.p > random.random():
      return 1.0
    else:
      return 0.0
  pass

#랜덤값 범위 확인
print([ random.random() for n in range(10) ])

"""## GameEngine Class
- 알고리즘 2개에 대한 표준 인터페이스 제공
"""

# 맴버변수의 파라미터들은 구현하면서 설정
# 알고리즘 검토후 필요변수 확인후 조정
class GameEngine():
  # 생성자(생략)
  # 알고리즘에 필요한 값 초기화
  def initialize(self):
    pass
  # Arm 선택
  def select_arm(self):
    pass
  # 1 action이 완료된후 정책조정 > 파라미터조정
  def policyUpdate(self):
    pass
  # 알고리즘 이름 출력
  def getName(self):
    pass

"""### ε-greedy 알고리즘

- 확률 ε(0~1)으로 랜덤하게 행동을 선택
  - arm을 선택하는 행위
- 확률 1-ε은 현재가치가 가장 높은 팔을 선택
- 이런 확률값중 가장 좋은 성능을 내는 값 0.1인 경우가 많다
- 0 ~ε: 탐색, ε ~ 1: 이용을 하였다
"""

from IPython.display import Image
Image('/content/drive/My Drive/data/DL_data_ref/ε-greedy.jpeg', width=700)
# 현재행동후 현재가치 = (처음부터 이전시도까지의 수행양)*이전번가치
#                       + (1/시행횟수)*이번시행보상

# 클래스로 구현
class EpsilonGreedyEngine(GameEngine):
  # 해당알고리즘에서 팔 선택시 랜덤 or 이미선택해본 팔중에 가치가 높은 팔을 
  # 선택할것이니 그것을 결정하는 기준값. 통상 0.1이 가장 좋은 성능을 냈다
  def __init__(self, epsilon): # `epsilon=0.1`을 넣어놓아도 됨
    self.epsilon = epsilon # 탐색하는 확률
    pass
  # 경험을 들고 있어야 하므로, 가치(보상) 및 시도
  def initialize(self, n_arms):
    self.n = np.zeros[n_arms] # 각 팔의 시행횟수[0,0,0]
    self.v = np.zeros[n_arms] # 각 팔의 가치    [0,0,0]
    pass
  # Arm을 선택
  def select_arm(self):
    if self.epsilon > random.random(): # 0.1보다 난수값이 작으면, 탐색
    # 랜덤하게 팔을 선택
    # 0 <= ~ < len(self.n) : 0 <= x < 3 : 0,1,2 중에 하나
    return np.random.randint( 0, len(self.n) )
      pass
    else: # 이용
      # 가치가 높은 팔의 index를 구해서 return. 팔번호:0,1,2중 하나
      return np.argmax( self.v )
      pass
    pass
  def policyUpdate(self, choice_arm, reward, ):
    # 1. 이번 에피소드에 선택한 Arm의 수행 횟수를 증가
    self.n[ choice_arm ] += 1
    # 2. 이번 에피소드에 선택한 Arm의 가치를 증가
    ## (n-1)/n * Vt - 1 + (1/n) * Rt
    n = self.n[ choice_arm ]
    # 갱신직전가치
    v = self.v[ choice_arm ]
    self.v[ choice_arm ] = ((n-1)/n) * v + (1/n) * reward
    pass
  def getName(self):
    return 'ε-greedy 알고리즘 이용한다'

# 게임시뮬레이터
# 1. 슬롯머신의 팔을 3개 준비. 
# 3개의 팔은 reward(1.0)을 지급하는 확률이 다르다
arms = [ SlotArm(0.3), SlotArm(0.5), SlotArm(0.9) ]
# 2개의 알고리즘 필요
algos = [ EpsilonGreedyEngine(0.1) ]



"""# 미로게임

- 알고리즘
  - 정책경사법
  - Sarsa
  - Q Learning

# 카트-풀 게임

- DQN
"""
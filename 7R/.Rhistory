setwd("C:/Repository/python_basic/7R")
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP)
final_data <- read.csv( "./Data/final_data.csv", encoding="UTF-8" )
txt <- str_replace_all(final_data[,4], "\\w", " " )
nouns <- extractNoun(txt)
library(dplyr)
library(stringr)
library( RColorBrewer ) # 칼라팔레트
library( wordcloud ) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\w", " " )
nouns <- extractNoun(txt)
str(txt)
head(nouns)
wordcount <- table(unlist(nouns))
setwd("./")
final_data <- read.csv( "./Data/final_data.csv", encoding="UTF-8" )
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP)
library(dplyr)
library(dplyr)
library(stringr)
library( RColorBrewer ) # 칼라팔레트
library( wordcloud ) # 워드클라우드 라이브러리
txt <- str_replace_all( final_data[,4], "\\w", " " )
str(txt)
nouns <- extractNoun(txt)
head(nouns)
wordcount <- table(unlist(nouns))
wordcount
## -------------------------------------------------------------------- ##
rm(list=ls(all=TRUE))
# 클리앙의 모두의 공원 게시글을 수집하는 코드
# html소스를 보면서 해야 이해가 빠름
# 긁고자 하는 사이트에서 오른쪽마우스클릭 -> 소스보기
# 규칙을 찾아 원하는 정보만 추출
install.packages('stringr')
library(stringr)
library(dplyr)
## -------------------------------------------------------------------- ##
final_data <- NULL # 초기화
for(i in 1:10) {
path  <- "https://www.clien.net/service/board/park?&od=T31&po="
url   <- paste0(path, i-1)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
## -------------------------------------------------------------------- ##
rm(list=ls(all=TRUE))
# 클리앙의 모두의 공원 게시글을 수집하는 코드
# html소스를 보면서 해야 이해가 빠름
# 긁고자 하는 사이트에서 오른쪽마우스클릭 -> 소스보기
# 규칙을 찾아 원하는 정보만 추출
install.packages('stringr')
library(stringr)
library(dplyr)
## -------------------------------------------------------------------- ##
final_data <- NULL # 초기화
for(i in 1:10) {
path  <- "https://www.clien.net/service/board/park?&od=T31&po="
url   <- paste0(path, i-1)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
# 공지글의 조회수가 포함되어 1,2 번쨰 데이터는 빼고 읽어옴
hit   <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
# hit
b4    <- str_split(b3, "hit\">")
hit   <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
# <a class> 추출 확인
b5    <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
# href 가져오기
b6    <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
# 글내용 접속 URL
url   <- paste0("https://www.clien.net", b6)
data  <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
tail(final_data)
# 글목록 데이터 저장
setwd("./")
write.csv( final_data, "./Data/base_data.csv", row.names=F )
## -------------------------------------------------------------------- ##
dim(final_data)
head(final_data)
tail(final_data)
# install.packages('stringr')
library(stringr)
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
b <- readLines(url, encoding="UTF-8")
b2 <- b[str_detect(b, "subject_fixed")]
title <- str_extract(b2, ("(?<=\">).*(?=</span>)")) ## page로 시작하고 </a> 끝나는 가운데걸 뽑습니다.
b3 <- b[str_detect(b, "<span class=\"hit\">")]
hit <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1]
b4 <- str_split(b3, "hit\">")
hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
b5 <- b[which(str_detect(b, "subject_fixed"))-2]
b6 <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
url <- paste0("https://www.clien.net", b6)
data <- cbind(title, hit, url)
final_data <- rbind(final_data, data)
cat("\n", i)
i <- 1
final_data <- NULL
for(i in 1:10) {
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
# url
b <- readLines(url, encoding="UTF-8")
# str(b)
# length(b)
# head(b)
b2 <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)")) ## page로 시작하고 </a> 끝나는 가운데걸 뽑습니다.
# str(title)
b3 <- b[str_detect(b, "<span class=\"hit\">")]
# str(b3)
hit <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1]
b4 <- str_split(b3, "hit\">")
str(b4)
hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
#hit
# b[which(str_detect(b, "subject_fixed"))]
# b[(str_detect(b, "subject_fixed"))]
b5 <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
b6 <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
url <- paste0("https://www.clien.net", b6)
data <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
tail(final_data)
setwd("./")
write.csv(final_data, "base_data1.csv", row.names=F)
# 글 본문 데이터를 이용하여 형태소 분석 및 워드 클라우드
setwd("./")
final_data <- read.csv("./Data/final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
#final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
head(nouns)
wordcount <- table(unlist(nouns))
wordcount
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word <- rename(df_word, word = Var1, freq = Freq)
str(df_word)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
top_20
?brewer.pal
pal <- brewer.pal(8,"Dark2")
set.seed(1234)
wordcloud(words = df_word$word, # 단어
freq = df_word$freq, # 빈도
min.freq = 2, # 최소 단어 빈도
max.words = 200, # 표현 단어 수
random.order = F, # 고빈도 단어 중앙 배치
rot.per = .1, # 회전 단어 비율
scale = c(4, 0.3), # 단어 크기 범위
colors = pal) # 색깔 목록
## -------------------------------------------------------------------- ##
rm(list=ls(all=TRUE))
# 클리앙의 모두의 공원 게시글을 수집하는 코드
# html소스를 보면서 해야 이해가 빠름
# 긁고자 하는 사이트에서 오른쪽마우스클릭 -> 소스보기
# 규칙을 찾아 원하는 정보만 추출
install.packages('stringr')
library(stringr)
library(dplyr)
## -------------------------------------------------------------------- ##
final_data <- NULL # 초기화
for(i in 1:10) {
path  <- "https://www.clien.net/service/board/park?&od=T31&po="
url   <- paste0(path, i-1)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
# 공지글의 조회수가 포함되어 1,2 번쨰 데이터는 빼고 읽어옴
hit   <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
# hit
b4    <- str_split(b3, "hit\">")
hit   <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
# <a class> 추출 확인
b5    <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
# href 가져오기
b6    <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
# 글내용 접속 URL
url   <- paste0("https://www.clien.net", b6)
data  <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
tail(final_data)
# 글목록 데이터 저장
setwd("./")
write.csv( final_data, "./Data/base_data.csv", row.names=F )
## -------------------------------------------------------------------- ##
## -------------------------------------------------------------------- ##
rm(list=ls(all=TRUE))
# 클리앙의 모두의 공원 게시글을 수집하는 코드
# html소스를 보면서 해야 이해가 빠름
# 긁고자 하는 사이트에서 오른쪽마우스클릭 -> 소스보기
# 규칙을 찾아 원하는 정보만 추출
install.packages('stringr')
library(stringr)
library(dplyr)
## -------------------------------------------------------------------- ##
final_data <- NULL # 초기화
for(i in 1:10) {
path  <- "https://www.clien.net/service/board/park?&od=T31&po="
url   <- paste0(path, i-1)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
# 공지글의 조회수가 포함되어 1,2 번쨰 데이터는 빼고 읽어옴
hit   <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
# hit
install.packages("stringr")
########클리앙의 모두의 공원 게시글을 수집하는 코드 입니다.
########
########크롤링은 html소스를 보면서 하셔야 이해가 빠릅니다.
########긁고자 하는 사이트에서 오른쪽 마우스 클릭-> 소스 보기를 하시면 소스를 보실수 있고
########그 소스내에 내가 원하는 정보가 있는 위치를 찾고, 규칙을 찾아 원하는 정보만 추출합니다.
######## 예를 들어 게시판의 제목이 <h2>제목</h2> 이렇게 만 나온다고한다면
######## <h2>가 있는 line만 찾아서 뽑으면 될것입니다.
######## 글 목록, 조회수, 글 본문 링크 정보 수집
# install.packages('stringr')
library(stringr)
i <- 1
final_data <- NULL
for(i in 1:10) {
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
# url
b <- readLines(url, encoding="UTF-8")
# str(b)
# length(b)
#head(b)
#tail(b)
b2 <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)")) ## page로 시작하고 </a> 끝나는 가운데걸 뽑습니다.
# str(title)
b3 <- b[str_detect(b, "<span class=\"hit\">")]
# str(b3)
hit <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1]
b4 <- str_split(b3, "hit\">")
str(b4)
hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
#hit
# b[which(str_detect(b, "subject_fixed"))]
# b[(str_detect(b, "subject_fixed"))]
b5 <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
b6 <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
url <- paste0("https://www.clien.net", b6)
data <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
tail(final_data)
setwd("./")
write.csv(final_data, "base_data.csv", row.names=F)
setwd("C:/Repository/python_basic/7R")
setwd("./")
write.csv(final_data, "final_data.csv", row.names=F)
######## 글 본문 데이터를 이용하여 형태소 분석 및 워드 클라우드
setwd("./")
final_data <- read.csv("./final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
final_data <- NULL
######## 글 목록, 조회수, 글 본문 링크 정보 수집
# install.packages('stringr')
library(stringr)
final_data <- NULL
for(i in 1:10) {
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
# url
b <- readLines(url, encoding="UTF-8")
# str(b)
# length(b)
#head(b)
#tail(b)
b2 <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)")) ## page로 시작하고 </a> 끝나는 가운데걸 뽑습니다.
# str(title)
b3 <- b[str_detect(b, "<span class=\"hit\">")]
# str(b3)
hit <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1]
b4 <- str_split(b3, "hit\">")
str(b4)
hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
#hit
# b[which(str_detect(b, "subject_fixed"))]
# b[(str_detect(b, "subject_fixed"))]
b5 <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
b6 <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
url <- paste0("https://www.clien.net", b6)
data <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
# str(b)
# length(b)
head(b)
cat("\n", i)
for(i in 1:1) {
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
# url
b <- readLines(url, encoding="UTF-8")
# str(b)
# length(b)
head(b)
#tail(b)
cat("\n", i)
}
url
######## 글 본문 데이터를 이용하여 형태소 분석 및 워드 클라우드
setwd("./")
final_data <- read.csv("./final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
#final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
head(nouns)
wordcount <- table(unlist(nouns))
wordcount
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word <- rename(df_word, word = Var1, freq = Freq)
str(df_word)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
top_20
?brewer.pal
pal <- brewer.pal(8,"Dark2")
set.seed(1234)
wordcloud(words = df_word$word, # 단어
freq = df_word$freq, # 빈도
min.freq = 2, # 최소 단어 빈도
max.words = 200, # 표현 단어 수
random.order = F, # 고빈도 단어 중앙 배치
rot.per = .1, # 회전 단어 비율
scale = c(4, 0.3), # 단어 크기 범위
colors = pal) # 색깔 목록
#final_data <- read.csv("./final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
library(KoNLP) # 한글 형태소 분석기
install.packages("rJava")
install.packages("KoNLP")
library(KoNLP) # 한글 형태소 분석기
install.packages("KoNLP")
install.packages("DBI")
install.packages("bit64")
install.packages("blob")
library(KoNLP) # 한글 형태소 분석기
library(KoNLP) # 한글 형태소 분석기
install.packages("rJava")
install.packages("DBI")
install.packages("bit64")
install.packages("blob")
#library(KoNLP)
#install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
library(KoNLP) # 한글 형태소 분석기
install.packages("rJava")
install.packages("DBI")
install.packages("bit64")
install.packages("blob")
#library(KoNLP)
#install.packages("KoNLP")
install.packages("wordcloud")
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
library(KoNLP) # 한글 형태소 분석기
txt <- str_replace_all(final_data[,4], "\\W", " ")

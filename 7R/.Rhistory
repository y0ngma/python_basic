# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
# 공지글의 조회수가 포함되어 1,2 번쨰 데이터는 빼고 읽어옴
hit   <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
# hit
install.packages("stringr")
########클리앙의 모두의 공원 게시글을 수집하는 코드 입니다.
########
########크롤링은 html소스를 보면서 하셔야 이해가 빠릅니다.
########긁고자 하는 사이트에서 오른쪽 마우스 클릭-> 소스 보기를 하시면 소스를 보실수 있고
########그 소스내에 내가 원하는 정보가 있는 위치를 찾고, 규칙을 찾아 원하는 정보만 추출합니다.
######## 예를 들어 게시판의 제목이 <h2>제목</h2> 이렇게 만 나온다고한다면
######## <h2>가 있는 line만 찾아서 뽑으면 될것입니다.
######## 글 목록, 조회수, 글 본문 링크 정보 수집
# install.packages('stringr')
library(stringr)
i <- 1
final_data <- NULL
for(i in 1:10) {
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
# url
b <- readLines(url, encoding="UTF-8")
# str(b)
# length(b)
#head(b)
#tail(b)
b2 <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)")) ## page로 시작하고 </a> 끝나는 가운데걸 뽑습니다.
# str(title)
b3 <- b[str_detect(b, "<span class=\"hit\">")]
# str(b3)
hit <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1]
b4 <- str_split(b3, "hit\">")
str(b4)
hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
#hit
# b[which(str_detect(b, "subject_fixed"))]
# b[(str_detect(b, "subject_fixed"))]
b5 <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
b6 <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
url <- paste0("https://www.clien.net", b6)
data <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
tail(final_data)
setwd("./")
write.csv(final_data, "base_data.csv", row.names=F)
setwd("C:/Repository/python_basic/7R")
setwd("./")
write.csv(final_data, "final_data.csv", row.names=F)
######## 글 본문 데이터를 이용하여 형태소 분석 및 워드 클라우드
setwd("./")
final_data <- read.csv("./final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
final_data <- NULL
######## 글 목록, 조회수, 글 본문 링크 정보 수집
# install.packages('stringr')
library(stringr)
final_data <- NULL
for(i in 1:10) {
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
# url
b <- readLines(url, encoding="UTF-8")
# str(b)
# length(b)
#head(b)
#tail(b)
b2 <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)")) ## page로 시작하고 </a> 끝나는 가운데걸 뽑습니다.
# str(title)
b3 <- b[str_detect(b, "<span class=\"hit\">")]
# str(b3)
hit <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1]
b4 <- str_split(b3, "hit\">")
str(b4)
hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
#hit
# b[which(str_detect(b, "subject_fixed"))]
# b[(str_detect(b, "subject_fixed"))]
b5 <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
b6 <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
url <- paste0("https://www.clien.net", b6)
data <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
# str(b)
# length(b)
head(b)
cat("\n", i)
for(i in 1:1) {
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
# url
b <- readLines(url, encoding="UTF-8")
# str(b)
# length(b)
head(b)
#tail(b)
cat("\n", i)
}
url
######## 글 본문 데이터를 이용하여 형태소 분석 및 워드 클라우드
setwd("./")
final_data <- read.csv("./final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
#final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
head(nouns)
wordcount <- table(unlist(nouns))
wordcount
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word <- rename(df_word, word = Var1, freq = Freq)
str(df_word)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
top_20
?brewer.pal
pal <- brewer.pal(8,"Dark2")
set.seed(1234)
wordcloud(words = df_word$word, # 단어
freq = df_word$freq, # 빈도
min.freq = 2, # 최소 단어 빈도
max.words = 200, # 표현 단어 수
random.order = F, # 고빈도 단어 중앙 배치
rot.per = .1, # 회전 단어 비율
scale = c(4, 0.3), # 단어 크기 범위
colors = pal) # 색깔 목록
#final_data <- read.csv("./final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
library(KoNLP) # 한글 형태소 분석기
install.packages("rJava")
install.packages("KoNLP")
library(KoNLP) # 한글 형태소 분석기
install.packages("KoNLP")
install.packages("DBI")
install.packages("bit64")
install.packages("blob")
library(KoNLP) # 한글 형태소 분석기
library(KoNLP) # 한글 형태소 분석기
install.packages("rJava")
install.packages("DBI")
install.packages("bit64")
install.packages("blob")
#library(KoNLP)
#install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
library(KoNLP) # 한글 형태소 분석기
install.packages("rJava")
install.packages("DBI")
install.packages("bit64")
install.packages("blob")
#library(KoNLP)
#install.packages("KoNLP")
install.packages("wordcloud")
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
library(KoNLP) # 한글 형태소 분석기
txt <- str_replace_all(final_data[,4], "\\W", " ")
# 클리앙의 모두의 공원 게시글을 수집하는 코드
# html소스를 보면서 해야 이해가 빠름
# 긁고자 하는 사이트에서 오른쪽마우스클릭 -> 소스보기
# 규칙을 찾아 원하는 정보만 추출
install.packages('stringr')
library(stringr)
library(dplyr)
## -------------------------------------------------------------------- ##
final_data <- NULL # 초기화
for(i in 1:10) {
path  <- "https://www.clien.net/service/board/park?&od=T31&po="
url   <- paste0(path, i-1)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
# 공지글의 조회수가 포함되어 1,2 번쨰 데이터는 빼고 읽어옴
hit   <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
# hit
b4    <- str_split(b3, "hit\">")
hit   <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
# <a class> 추출 확인
b5    <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
# href 가져오기
b6    <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
# 글내용 접속 URL
url   <- paste0("https://www.clien.net", b6)
data  <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
## -------------------------------------------------------------------- ##
rm(list=ls(all=TRUE))
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
# 공지글의 조회수가 포함되어 1,2 번쨰 데이터는 빼고 읽어옴
hit   <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
# hit
b4    <- str_split(b3, "hit\">")
hit   <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
# <a class> 추출 확인
b5    <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
# href 가져오기
b6    <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
# 글내용 접속 URL
url   <- paste0("https://www.clien.net", b6)
data  <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
length(source)
str(source)
str(source)
length(source)
head(source, 10)
head(source, 100)
# str(source)
length(source)
str(b2)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
str(b2)
head(b2, 10)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
str(b2)
head(b2, 10)
}
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
head(b2, 10)
}
head(b2)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUR-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
head(b2, 10)
}
# str(b2)
head(b2, 10)
# str(b2)
b3<-repair_encoding(b2)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
}
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</strong>)"))
# str(title)
}
str(title)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
}
head(b2, 10)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUC-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
}
head(b2, 10)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUC-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</strong>)"))
}
str(title)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUC-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class=\"tit\"")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</strong>)"))
str(title)
}
str(title)
title
library(rvest)
install.packages('rvest')
install.packages("rvest")
# 글 본문 데이터를 이용하여 형태소 분석 및 워드 클라우드
setwd("./")
final_data <- read.csv("./Data/final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
#final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
head(nouns)
wordcount <- table(unlist(nouns))
wordcount
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word <- rename(df_word, word = Var1, freq = Freq)
str(df_word)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
top_20
?brewer.pal
pal <- brewer.pal(8,"Dark2")
set.seed(1234)
wordcloud(words = df_word$word, # 단어
freq = df_word$freq, # 빈도
min.freq = 2, # 최소 단어 빈도
max.words = 200, # 표현 단어 수
random.order = F, # 고빈도 단어 중앙 배치
rot.per = .1, # 회전 단어 비율
scale = c(4, 0.3), # 단어 크기 범위
colors = pal) # 색깔 목록

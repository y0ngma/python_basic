Encoding(tem)
tem_utf <- iconv(tem, to="UTF-8")
tem-utf
Encoding(tem_utf)
tem_utf
Encoding(tem_utf)
tem_utf
Encoding(tem) <- "UTF-8"
tem
wifi %>%
distinct(설치시군구명) %>%
slice(1:10) %>%
transmute( iconv_utf8=str_conv(설치시군구명, "utf8"),
iconv_cp949=str_conv(설치시군구명, "cp949") )
wifi %>%
with(Encoding(설치시군구명) %>%
table() %>%
as_tibble() )
wifi %>%
with( Encoding(설치시군구명) %>%
table() %>%
as_tibble() )
wifi %>%
distinct(설치시군구명) %>%
filter( Encoding(설치시군구명)=="unknown" )
# 한글인코딩 (윈도우용)
# cp949(ms949라고도 불림. 웹에서 euc-kr도 있음)와 utf-8(UTF-8)
# 인코딩은 데이터자체인 바이트단위기록 그자체와 해석법이라는 메타데이터로 이루어짐
tem <- "안녕 하세요"
tem
tem
tem_utf <- iconv(tem, to="UTF-8")
tem-utf
tem-utf
tem_utf
Encoding(tem_utf)
Encoding(tem)
Encoding(tem_utf)
tem_utf
tem_utf
tem_utf
rm(list=ls(all=TRUE)) # 메모리에 로딩되어있는 모든 변수 삭제
tem <- "안녕 하세요"
tem
Encoding(tem)
tem
tem
Encoding(tem) <- "UTF-8"
install.packages('stringr')
library(stringr)
final_data <- NULL
library(stringr)
install.packages('stringr')
install.packages("stringr")
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
i <- 1
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
## -------------------------------------------------------------------- ##
rm(list=ls(all=TRUE)) # 메모리에 로딩되어있는 모든 변수 삭제
install.packages('stringr')
library(stringr)
install.packages("stringr")
Encoding(tem_utf)
install.packages("stringr")
b <- readLines(url, encoding="UTF-8")
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
b <- readLines(url, encoding="UTF-8")
i <- 1
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
b <- readLines(url, encoding="UTF-8")
url
b <- readLines(url, encoding="UTF-8")
# str(b)
length(b)
lead(b)
sourcepage <- readLines(url, encoding="UTF-8")
# str(sourcepage)
length(sourcepage)
lead(sourcepage)
head(sourcepage)
head(sourcepage, 10)
str(sourcepage)
str(sourcepage)
head(sourcepage, 9)
sourcepage2 <- sourcepage[ str_detect(source, "subject_fixed") ]
sourcepage2 <- sourcepage[ str_detect(source, "subject_fixed") ]
sourcepage2 <- sourcepage[ str_detect(sourcepage, "subject_fixed") ]
sourcepage2 <- sourcepage[ str_detect(sourcepage, "subject_fixed") ]
sourcepage2 <- sourcepage[ str_detect(sourcepage, "subject_fixed") ]
source <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면 <span class="subject_fixed"...
source2 <- source[ str_detect(source, "subject_fixed") ]
str(source2)
# </span>사이의 글을 읽어 와라
title <- str_extract( source2, ("(?<=">).*(?=</span>)") )
source3 <- source[str_detect(source, "<span class=\"hit\">") ]
library(dplyr)
# </span>사이의 글을 읽어 와라
title <- str_extract( source2, ("(?<=">).*(?=</span>)") )
source3 <- source[str_detect(source, "<span class=\"hit\">") ]
library(dplyr)
final_data <- NULL
i <- 1
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
url
source <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
head(source, 10)
source2 <- source[ str_detect(source, "subject_fixed") ]
b2 <- source[ str_detect(source, "subject_fixed") ]
library(stringr)
('stringr')
('stringr')
install.packages('stringr')
install.packages("stringr")
library(stringr)
library(dplyr)
final_data <- NULL
i <- 1
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
url
source <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
head(source, 10)
b2 <- source[ str_detect(source, "subject_fixed") ]
str(b2)
# </span>사이의 글을 읽어 와라
title <- str_extract( b2, ("(?<=\">).*(?=</span>)") )[-1]
#  \" 는 을 문자열로 인식하게 함
b3 <- source[str_detect(source, "<span class=\"hit\">") ]
b3 <- source[str_detect(source, '<span class="hit">') ] # 동일결과
b3
# </span>사이의 글을 읽어 와라
title <- str_extract( b2, ("(?<=\">).*(?=</span>)") )[-1]
title
b2 <- source[ str_detect(source, "subject_fixed") ]
str(b2)
str(title)
str(b3)
#  \" 는 을 문자열로 인식하게 함
hit <- source[str_detect(source, "<span class=\"hit\">") ]
hit <- source[str_detect(source, '<span class="hit">') ] # 동일결과
str(hit)
head(hit)
hit <- source[str_detect(source, "<span class=\"hit\">") ]
head(hit)
hit <- str_extract( b3, ("(?<=\">).*(?=</span>)") )[-1]
head(hit)
head(hit[2:-1])
b4
b4 <- str_split(b3, "hit\">")
b4
# b4 <- str_split(b3, "hit\">")
# b4
hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8 )
head(hit[2:-1])
head(hit)
head(hit[-1])
head(hit)
hit <- str_extract( b3, ("(?<=\">).*(?=</span>)") )[-1:-2]
# b4 <- str_split(b3, "hit\">")
# b4
# hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8 )
head(hit)
# b4 <- str_split(b3, "hit\">")
# b4
# hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8 )
head(hit,3)
head( b[which(str_detect(b, "subject_fixed"))] )
head( source[which(str_detect(source, "subject_fixed"))] )
head( source[which(str_detect(source, "subject_fixed"))-2],3 )
str(b5)
b5 <- source[which(str_detect(source, "subject_fixed"))-2]
str(b5)
b6 <- str_sub( str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=4 )
str(b6)
url <- paste0("https://www.clien.net", b6)
head(url,3)
data <- cbind(title, hit, url)
data
b6<-str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
for(i in 1:10){
url<-paste0('https://www.clien.net/service/board/park?&od=T31&po=',i-1)
b3<-b[str_detect(b,"<span class=\"hit\">")]
title<-str_extract(b2, ("(?<=\">).*(?=</span>)"))
b3<-b[str_detect(b,"<span class=\"hit\">")]
hit<-str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
head(b[which(str_detect(b,"subject_fixed"))-2],3)
b5<-b[which(str_detect(b,"subject_fixed"))-2]
b6<-str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
url<-paste0("https://www.clien.net",b6)
data<-cbind(title, hit, url)
final_data<- rbind(final_data,data)
}
for(i in 1:10){
url<-paste0('https://www.clien.net/service/board/park?&od=T31&po=',i-1)
b3<-b[str_detect(b,"<span class=\"hit\">")]
title<-str_extract(b2, ("(?<=\">).*(?=</span>)"))
b3<-b[str_detect(b,"<span class=\"hit\">")]
hit<-str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
head(b[which(str_detect(b,"subject_fixed"))-2],3)
b5<-b[which(str_detect(b,"subject_fixed"))-2]
b6<-str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
url<-paste0("https://www.clien.net",b6)
data<-cbind(title, hit, url)
final_data<- rbind(final_data,data)
}
i <- 1
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
url
path <- "https://www.clien.net/service/board/park?&od=T31&po="
url <- paste0(path, i-1)
url
source <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
head(source, 10)
b2 <- source[ str_detect(source, "subject_fixed") ]
## -------------------------------------------------------------------- ##
instal.packages("rJava")
## -------------------------------------------------------------------- ##
install.packages("rJava")
if ( !require("devtools") ) install.packages('devtools')
devtools::install_github('haven-jeon/KoLNP')
library(RJSONIO)
keyword = '부산 대학교 캠퍼스'
install.packages("RJSONIO")
library(RJSONIO)
keyword = '부산 대학교 캠퍼스'
loc <- iconv( keyword, from="cp949", to="UTF-8" ) # 윈도 키워드인코딩 변화
juso <- URLencode(loc)
url<-paste0("https://m.map.naver.com/search2/searchMore.nhn?query=",
juso,"&sm=clk&page=1&displayCount=75&type=SITE_1")
url
b <- readLines( url, encoding="UTF-8" )
b2 <- paste( b, collapse=" " ) #JSON형태라서 R에서 list로 변환
head(b2[1])
b3 <- fromJSON(b2) # list로 변환
head(b3)[1]
b2 <- paste( b, collapse=" " ) #JSON형태라서 R에서 list로 변환
head(b2[1])
loc <- iconv( keyword, from="cp949", to="UTF-8" ) # 윈도 키워드인코딩 변화
juso <- URLencode(loc)
search_loc <- function(keyword){
loc <- iconv( keyword, from="cp949", to="UTF-8" ) # 윈도 키워드인코딩 변화
juso <- URLencode(loc)
url<-paste0("https://m.map.naver.com/search2/searchMore.nhn?query=",
juso,"&sm=clk&page=1&displayCount=75&type=SITE_1")
b <- readLines( url, encoding="UTF-8" )
b2 <- paste( b, collapse=" " ) #JSON형태라서 R에서 list로 변환
b3 <- fromJSON(b2) # list로 변환
# x, y 좌표 찾기
# JSON 요소는 $으로 접근가능
wg <- c( b3$result$site$list[[1]]$x, b3$result$site$list[[1]]$y )
return(wg)
}
search_loc(서울대)
search_loc('서울대')
search_address('서울대')
search_address('부ㅠ산산대')
search_address('부산대')
<-
search_address <- function(keyword){
loc <- iconv( keyword, from="cp949", to="UTF-8" ) # 윈도 키워드인코딩 변화
juso <- URLencode(loc)
url<-paste0("https://m.map.naver.com/search2/searchMore.nhn?query=",
juso,"&sm=clk&page=1&displayCount=75&type=SITE_1")
b <- readLines( url, encoding="UTF-8" )
b2 <- paste( b, collapse=" " ) #JSON형태라서 R에서 list로 변환
b3 <- fromJSON(b2) # list로 변환
# 주소 찾기
wg <- c( b3$result$site$list[[1]]$address )
return(wg)
}
search_address('부산대')
search_loc('부산대')
search_loc <- function(keyword){
loc <- iconv( keyword, from="cp949", to="UTF-8" ) # 윈도 키워드인코딩 변화
juso <- URLencode(loc)
url<-paste0("https://m.map.naver.com/search2/searchMore.nhn?query=",
juso,"&sm=clk&page=1&displayCount=75&type=SITE_1")
b <- readLines( url, encoding="UTF-8" )
b2 <- paste( b, collapse=" " ) #JSON형태라서 R에서 list로 변환
b3 <- fromJSON(b2) # list로 변환
# x, y 좌표 찾기
# JSON 요소는 $으로 접근가능
wg <- c( b3$result$site$list[[1]]$x, b3$result$site$list[[1]]$y )
return(wg)
}
search_loc('부산대')
b2 <- paste( b, collapse=" " ) #JSON형태라서 R에서 list로 변환
head(b2[1])
b3 <- fromJSON(b2) # list로 변환
head(b3)[1]
setwd("C:/Repository/python_basic/7R")
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP)
final_data <- read.csv( "./Data/final_data.csv", encoding="UTF-8" )
txt <- str_replace_all(final_data[,4], "\\w", " " )
nouns <- extractNoun(txt)
library(dplyr)
library(stringr)
library( RColorBrewer ) # 칼라팔레트
library( wordcloud ) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\w", " " )
nouns <- extractNoun(txt)
str(txt)
head(nouns)
wordcount <- table(unlist(nouns))
setwd("./")
final_data <- read.csv( "./Data/final_data.csv", encoding="UTF-8" )
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP)
library(dplyr)
library(dplyr)
library(stringr)
library( RColorBrewer ) # 칼라팔레트
library( wordcloud ) # 워드클라우드 라이브러리
txt <- str_replace_all( final_data[,4], "\\w", " " )
str(txt)
nouns <- extractNoun(txt)
head(nouns)
wordcount <- table(unlist(nouns))
wordcount
## -------------------------------------------------------------------- ##
rm(list=ls(all=TRUE))
# 클리앙의 모두의 공원 게시글을 수집하는 코드
# html소스를 보면서 해야 이해가 빠름
# 긁고자 하는 사이트에서 오른쪽마우스클릭 -> 소스보기
# 규칙을 찾아 원하는 정보만 추출
install.packages('stringr')
library(stringr)
library(dplyr)
## -------------------------------------------------------------------- ##
final_data <- NULL # 초기화
for(i in 1:10) {
path  <- "https://www.clien.net/service/board/park?&od=T31&po="
url   <- paste0(path, i-1)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
## -------------------------------------------------------------------- ##
rm(list=ls(all=TRUE))
# 클리앙의 모두의 공원 게시글을 수집하는 코드
# html소스를 보면서 해야 이해가 빠름
# 긁고자 하는 사이트에서 오른쪽마우스클릭 -> 소스보기
# 규칙을 찾아 원하는 정보만 추출
install.packages('stringr')
library(stringr)
library(dplyr)
## -------------------------------------------------------------------- ##
final_data <- NULL # 초기화
for(i in 1:10) {
path  <- "https://www.clien.net/service/board/park?&od=T31&po="
url   <- paste0(path, i-1)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
# 공지글의 조회수가 포함되어 1,2 번쨰 데이터는 빼고 읽어옴
hit   <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
# hit
b4    <- str_split(b3, "hit\">")
hit   <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
# <a class> 추출 확인
b5    <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
# href 가져오기
b6    <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
# 글내용 접속 URL
url   <- paste0("https://www.clien.net", b6)
data  <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
tail(final_data)
# 글목록 데이터 저장
setwd("./")
write.csv( final_data, "./Data/base_data.csv", row.names=F )
## -------------------------------------------------------------------- ##
dim(final_data)
head(final_data)
tail(final_data)
# install.packages('stringr')
library(stringr)
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
b <- readLines(url, encoding="UTF-8")
b2 <- b[str_detect(b, "subject_fixed")]
title <- str_extract(b2, ("(?<=\">).*(?=</span>)")) ## page로 시작하고 </a> 끝나는 가운데걸 뽑습니다.
b3 <- b[str_detect(b, "<span class=\"hit\">")]
hit <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1]
b4 <- str_split(b3, "hit\">")
hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
b5 <- b[which(str_detect(b, "subject_fixed"))-2]
b6 <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
url <- paste0("https://www.clien.net", b6)
data <- cbind(title, hit, url)
final_data <- rbind(final_data, data)
cat("\n", i)
i <- 1
final_data <- NULL
for(i in 1:10) {
url <- paste0("https://www.clien.net/service/board/park?&od=T31&po=", i-1)
# url
b <- readLines(url, encoding="UTF-8")
# str(b)
# length(b)
# head(b)
b2 <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)")) ## page로 시작하고 </a> 끝나는 가운데걸 뽑습니다.
# str(title)
b3 <- b[str_detect(b, "<span class=\"hit\">")]
# str(b3)
hit <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1]
b4 <- str_split(b3, "hit\">")
str(b4)
hit <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
#hit
# b[which(str_detect(b, "subject_fixed"))]
# b[(str_detect(b, "subject_fixed"))]
b5 <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
b6 <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
url <- paste0("https://www.clien.net", b6)
data <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
tail(final_data)
setwd("./")
write.csv(final_data, "base_data1.csv", row.names=F)
# 글 본문 데이터를 이용하여 형태소 분석 및 워드 클라우드
setwd("./")
final_data <- read.csv("./Data/final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
#final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
head(nouns)
wordcount <- table(unlist(nouns))
wordcount
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word <- rename(df_word, word = Var1, freq = Freq)
str(df_word)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
top_20
?brewer.pal
pal <- brewer.pal(8,"Dark2")
set.seed(1234)
wordcloud(words = df_word$word, # 단어
freq = df_word$freq, # 빈도
min.freq = 2, # 최소 단어 빈도
max.words = 200, # 표현 단어 수
random.order = F, # 고빈도 단어 중앙 배치
rot.per = .1, # 회전 단어 비율
scale = c(4, 0.3), # 단어 크기 범위
colors = pal) # 색깔 목록

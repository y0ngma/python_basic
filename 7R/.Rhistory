b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</strong>)"))
# str(title)
}
str(title)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
}
head(b2, 10)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUC-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
}
head(b2, 10)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUC-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</strong>)"))
}
str(title)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUC-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class=\"tit\"")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</strong>)"))
str(title)
}
str(title)
title
library(rvest)
install.packages('rvest')
install.packages("rvest")
# 글 본문 데이터를 이용하여 형태소 분석 및 워드 클라우드
setwd("./")
final_data <- read.csv("./Data/final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
#final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
head(nouns)
wordcount <- table(unlist(nouns))
wordcount
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word <- rename(df_word, word = Var1, freq = Freq)
str(df_word)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
top_20
?brewer.pal
pal <- brewer.pal(8,"Dark2")
set.seed(1234)
wordcloud(words = df_word$word, # 단어
freq = df_word$freq, # 빈도
min.freq = 2, # 최소 단어 빈도
max.words = 200, # 표현 단어 수
random.order = F, # 고빈도 단어 중앙 배치
rot.per = .1, # 회전 단어 비율
scale = c(4, 0.3), # 단어 크기 범위
colors = pal) # 색깔 목록
?lm
lm
require(graphics)
## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1) # omitting intercept
anova(lm.D9)
summary(lm.D90)
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(lm.D9, las = 1)      # Residuals, Fitted, ...
par(opar)
y <-runif(100)
x1 <-runif(100)
x2 <-runif(100)
x3 <-runif(100)
x4 <-runif(100)
x5 <-runif(100)
x6 <-runif(100)
data <- cbind(y,x1,x2,x3,x4,x5,x6)
data2 <- data.frame(data)
head(data2)
fit<-lm(y~., data=data2) #y를 뺀 모두
anova(fit)
summary(fit)
head(data2)
anova(fit)
summary(fit)
## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1) # omitting intercept
anova(lm.D9)
summary(lm.D90)
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(lm.D9, las = 1)      # Residuals, Fitted, ...
par(opar)
y <-runif(100)
x1 <-runif(100)
x2 <-runif(100)
x3 <-runif(100)
x4 <-runif(100)
x5 <-runif(100)
x6 <-runif(100)
data <- cbind(y,x1,x2,x3,x4,x5,x6)
data2 <- data.frame(data)
head(data2)
fit<-lm(y~., data=data2) #y를 뺀 모두
anova(fit)
summary(fit)
require(graphics)
data(cars)
cars
fit <- lm(dist!.,data=cars)
anova(fit)
summary(fit)
fit$coefficients[2] # w
fit$coefficients[1] # b
cars
fit <- lm(dist~.,data=cars)
anova(fit)
summary(fit)
?anova(fit)
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
train <- read.csv('../input/train.csv', stringsAsFactors = F)
---
title: 'Exploring the Titanic Dataset'
author: 'Megan L. Risdal'
date: '6 March 2016'
output:
html_document:
number_sections: true
toc: true
fig_width: 7
fig_height: 4.5
theme: readable
highlight: tango
---
---
title: 'Exploring the Titanic Dataset'
author: 'Megan L. Risdal'
date: '6 March 2016'
output:
html_document:
number_sections: true
toc: true
fig_width: 7
fig_height: 4.5
theme: readable
highlight: tango
---
# Introduction
This is my first stab at a Kaggle script. I have chosen to work with the Titanic dataset after spending some time poking around on the site and looking at other scripts made by other Kagglers for inspiration. I will also focus on doing some illustrative data visualizations along the way. I'll then use `randomForest` to create a model predicting survival on the Titanic. I am new to machine learning and hoping to learn a lot, so feedback is very welcome!
There are three parts to my script as follows:
* Feature engineering
* Missing value imputation
* Prediction!
## Load and check data
```{r, message = FALSE}
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
library('randomForest') # classification algorithm
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
Now that our packages are loaded, let's read in and take a peek at the data.
```{r, message=FALSE, warning=FALSE}
train <- read.csv('../input/train.csv', stringsAsFactors = F)
test  <- read.csv('../input/test.csv', stringsAsFactors = F)
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
# Load packages
library('ggplot2') # visualization
#library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
# Load packages
library('ggplot2') # visualization
#library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
#library('mice') # imputation
library('randomForest') # classification algorithm
#library('mice') # imputation
library('randomForest') # classification algorithm
# Load packages
library('ggplot2') # visualization
#library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
#library('mice') # imputation
library('randomForest') # classification algorithm
Now that our packages are loaded, let's read in and take a peek at the data.
```{r, message=FALSE, warning=FALSE}
train <- read.csv('../input/train.csv', stringsAsFactors = F)
test  <- read.csv('../input/test.csv', stringsAsFactors = F)
full  <- bind_rows(train, test) # bind training & test data
# check data
str(full)
```{r, message = FALSE}
# Load packages
library('ggplot2') # visualization
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
## Load and check data
install.packages("ggthemes")
```{r, message = FALSE}
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
library('randomForest') # classification algorithm
```
Now that our packages are loaded, let's read in and take a peek at the data.
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
## Load and check data
install.packages("ggthemes")
install.packages("mice")
```{r, message = FALSE}
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
library('randomForest') # classification algorithm
```
install.packages("ggthemes")
# Load packages
install.packages("ggthemes")
install.packages("mice")
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
library('randomForest') # classification algorithm
train <- read.csv('../input/train.csv', stringsAsFactors = F)
test  <- read.csv('../input/test.csv', stringsAsFactors = F)
full  <- bind_rows(train, test) # bind training & test data
# check data
str(full)
The first variable which catches my attention is **passenger name** because we can break it down into additional meaningful variables which can feed predictions or be used in the creation of additional new variables. For instance, **passenger title** is contained within the passenger name variable and we can use **surname** to represent families. Let's do some **feature engineering**!
```{r, message=FALSE, warning=FALSE}
# Grab title from passenger names
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
# Grab title from passenger names
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
# Show title counts by sex
table(full$Sex, full$Title)
# Titles with very low cell counts to be combined to "rare" level
rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don',
'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
# Also reassign mlle, ms, and mme accordingly
full$Title[full$Title == 'Mlle']        <- 'Miss'
full$Title[full$Title == 'Ms']          <- 'Miss'
full$Title[full$Title == 'Mme']         <- 'Mrs'
full$Title[full$Title %in% rare_title]  <- 'Rare Title'
# Show title counts by sex again
table(full$Sex, full$Title)
# Finally, grab surname from passenger name
full$Surname <- sapply(full$Name,
function(x) strsplit(x, split = '[,.]')[[1]][1])
# Grab title from passenger names
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
# Show title counts by sex
table(full$Sex, full$Title)
# Titles with very low cell counts to be combined to "rare" level
rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don',
'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
# Also reassign mlle, ms, and mme accordingly
full$Title[full$Title == 'Mlle']        <- 'Miss'
full$Title[full$Title == 'Ms']          <- 'Miss'
full$Title[full$Title == 'Mme']         <- 'Mrs'
full$Title[full$Title %in% rare_title]  <- 'Rare Title'
# Show title counts by sex again
table(full$Sex, full$Title)
# Finally, grab surname from passenger name
full$Surname <- sapply(full$Name,
function(x) strsplit(x, split = '[,.]')[[1]][1])
cat(paste('We have <b>', nlevels(factor(full$Surname)), '</b> unique surnames. I would be interested to infer ethnicity based on surname --- another time.'))
# Create a family size variable including the passenger themselves
full$Fsize <- full$SibSp + full$Parch + 1
# Create a family variable
full$Family <- paste(full$Surname, full$Fsize, sep='_')
# Use ggplot2 to visualize the relationship between family size & survival
ggplot(full[1:891,], aes(x = Fsize, fill = factor(Survived))) +
geom_bar(stat='count', position='dodge') +
scale_x_continuous(breaks=c(1:11)) +
labs(x = 'Family Size') +
theme_few()
# Discretize family size
full$FsizeD[full$Fsize == 1] <- 'singleton'
full$FsizeD[full$Fsize < 5 & full$Fsize > 1] <- 'small'
full$FsizeD[full$Fsize > 4] <- 'large'
# Show family size by survival using a mosaic plot
mosaicplot(table(full$FsizeD, full$Survived), main='Family Size by Survival', shade=TRUE)
# This variable appears to have a lot of missing values
full$Cabin[1:28]
# The first character is the deck. For example:
strsplit(full$Cabin[2], NULL)[[1]]
# Create a Deck variable. Get passenger deck A - F:
full$Deck<-factor(sapply(full$Cabin, function(x) strsplit(x, NULL)[[1]][1]))
# Passengers 62 and 830 are missing Embarkment
full[c(62, 830), 'Embarked']
cat(paste('We will infer their values for **embarkment** based on present data that we can imagine may be relevant: **passenger class** and **fare**. We see that they paid<b> $', full[c(62, 830), 'Fare'][[1]][1], '</b>and<b> $', full[c(62, 830), 'Fare'][[1]][2], '</b>respectively and their classes are<b>', full[c(62, 830), 'Pclass'][[1]][1], '</b>and<b>', full[c(62, 830), 'Pclass'][[1]][2], '</b>. So from where did they embark?'))
# Get rid of our missing passenger IDs
embark_fare <- full %>%
filter(PassengerId != 62 & PassengerId != 830)
# Use ggplot2 to visualize embarkment, passenger class, & median fare
ggplot(embark_fare, aes(x = Embarked, y = Fare, fill = factor(Pclass))) +
geom_boxplot() +
geom_hline(aes(yintercept=80),
colour='red', linetype='dashed', lwd=2) +
scale_y_continuous(labels=dollar_format()) +
theme_few()
# Since their fare was $80 for 1st class, they most likely embarked from 'C'
full$Embarked[c(62, 830)] <- 'C'
# Show row 1044
full[1044, ]
ggplot(full[full$Pclass == '3' & full$Embarked == 'S', ],
aes(x = Fare)) +
geom_density(fill = '#99d6ff', alpha=0.4) +
geom_vline(aes(xintercept=median(Fare, na.rm=T)),
colour='red', linetype='dashed', lwd=1) +
scale_x_continuous(labels=dollar_format()) +
theme_few()
# Replace missing fare value with median fare for class/embarkment
full$Fare[1044] <- median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm = TRUE)
# Show number of missing Age values
sum(is.na(full$Age))
# Make variables factors into factors
factor_vars <- c('PassengerId','Pclass','Sex','Embarked',
'Title','Surname','Family','FsizeD')
full[factor_vars] <- lapply(full[factor_vars], function(x) as.factor(x))
# Set a random seed
set.seed(129)
# Perform mice imputation, excluding certain less-than-useful variables:
mice_mod <- mice(full[, !names(full) %in% c('PassengerId','Name','Ticket','Cabin','Family','Surname','Survived')], method='rf')
# Save the complete output
mice_output <- complete(mice_mod)
knitr::opts_chunk$set(echo = TRUE)
# Data input, assesment : 데이터 불러들이기, 확인하는 과정
library(readr)           # Data input with readr::read_csv()
library(descr)           # descr::CrossTable() - 범주별 빈도수, 비율 수치로 확인
<br>
install.packages("descr")
```{r message=FALSE, warning=FALSE}
# Data input, assesment : 데이터 불러들이기, 확인하는 과정
library(readr)           # Data input with readr::read_csv()
library(descr)           # descr::CrossTable() - 범주별 빈도수, 비율 수치로 확인
# Data input, assesment : 데이터 불러들이기, 확인하는 과정
library(readr)           # Data input with readr::read_csv()
library(descr)           # descr::CrossTable() - 범주별 빈도수, 비율 수치로 확인
# Visualization
library(VIM)             # Missing values assesment used by VIM::aggr()
<br>
install.packages("descr")
install.packages("VIM")
<br>
install.packages("descr")
install.packages("VIM")
knitr::opts_chunk$set(echo = TRUE)
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist = NULL, file, cols = 1, layout = NULL) {
library(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
# Data input, assesment : 데이터 불러들이기, 확인하는 과정
library(readr)           # Data input with readr::read_csv()
library(descr)           # descr::CrossTable() - 범주별 빈도수, 비율 수치로 확인
# Visualization
library(VIM)             # Missing values assesment used by VIM::aggr()
library(ggplot2)         # Used in almost visualization
library(RColorBrewer)    # plot의 color 설정
library(scales)          # plot setting - x, y 축 설정
# Feature engineering, Data Pre-processing
# library(tidyverse)     # dplyr, ggplot2, purrr, etc...
library(dplyr)           # Feature Engineering & Data Pre-processing
library(purrr)           # Check missing values
library(tidyr)           # tidyr::gather()
# Model generation
library(randomForest)    # For Random Forest Modeling
# Model validation : 원래는 하는게 맞지만 이번 과정에서는 생략
# library(caret)           # caret::confusionMatrix()
# library(ROCR)            # Plotting ROC Curve
# Data input, assesment : 데이터 불러들이기, 확인하는 과정
library(readr)           # Data input with readr::read_csv()
library(descr)           # descr::CrossTable() - 범주별 빈도수, 비율 수치로 확인
# Visualization
library(VIM)             # Missing values assesment used by VIM::aggr()
library(ggplot2)         # Used in almost visualization
library(RColorBrewer)    # plot의 color 설정
library(scales)          # plot setting - x, y 축 설정
# Feature engineering, Data Pre-processing
# library(tidyverse)     # dplyr, ggplot2, purrr, etc...
library(dplyr)           # Feature Engineering & Data Pre-processing
library(purrr)           # Check missing values
library(tidyr)           # tidyr::gather()
# Model generation
library(randomForest)    # For Random Forest Modeling
# Model validation : 원래는 하는게 맞지만 이번 과정에서는 생략
# library(caret)           # caret::confusionMatrix()
# library(ROCR)            # Plotting ROC Curve

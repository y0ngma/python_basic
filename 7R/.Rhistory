pal <- brewer.pal(8,"Dark2")
set.seed(1234)
wordcloud(words = df_word$word, # 단어
freq = df_word$freq, # 빈도
min.freq = 2, # 최소 단어 빈도
max.words = 200, # 표현 단어 수
random.order = F, # 고빈도 단어 중앙 배치
rot.per = .1, # 회전 단어 비율
scale = c(4, 0.3), # 단어 크기 범위
colors = pal) # 색깔 목록
#final_data <- read.csv("./final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
library(KoNLP) # 한글 형태소 분석기
install.packages("rJava")
install.packages("KoNLP")
library(KoNLP) # 한글 형태소 분석기
install.packages("KoNLP")
install.packages("DBI")
install.packages("bit64")
install.packages("blob")
library(KoNLP) # 한글 형태소 분석기
library(KoNLP) # 한글 형태소 분석기
install.packages("rJava")
install.packages("DBI")
install.packages("bit64")
install.packages("blob")
#library(KoNLP)
#install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
library(KoNLP) # 한글 형태소 분석기
install.packages("rJava")
install.packages("DBI")
install.packages("bit64")
install.packages("blob")
#library(KoNLP)
#install.packages("KoNLP")
install.packages("wordcloud")
library(dplyr) # 데이터ㅓ프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
library(KoNLP) # 한글 형태소 분석기
txt <- str_replace_all(final_data[,4], "\\W", " ")
# 클리앙의 모두의 공원 게시글을 수집하는 코드
# html소스를 보면서 해야 이해가 빠름
# 긁고자 하는 사이트에서 오른쪽마우스클릭 -> 소스보기
# 규칙을 찾아 원하는 정보만 추출
install.packages('stringr')
library(stringr)
library(dplyr)
## -------------------------------------------------------------------- ##
final_data <- NULL # 초기화
for(i in 1:10) {
path  <- "https://www.clien.net/service/board/park?&od=T31&po="
url   <- paste0(path, i-1)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
# 공지글의 조회수가 포함되어 1,2 번쨰 데이터는 빼고 읽어옴
hit   <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
# hit
b4    <- str_split(b3, "hit\">")
hit   <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
# <a class> 추출 확인
b5    <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
# href 가져오기
b6    <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
# 글내용 접속 URL
url   <- paste0("https://www.clien.net", b6)
data  <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
## -------------------------------------------------------------------- ##
rm(list=ls(all=TRUE))
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
# length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <span class="subject_fixed" data-role="list-title-text" title="오늘..
b2    <- b[str_detect(b, "subject_fixed")]
# str(b2)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
#  \" 는 을 문자열로 인식하게 함
b3    <- b[str_detect(b, '<span class="hit">') ]
b3    <- b[str_detect(b, "<span class=\"hit\">")] # 동일결과
# str(b3)
# 공지글의 조회수가 포함되어 1,2 번쨰 데이터는 빼고 읽어옴
hit   <- str_extract(b3, ("(?<=\">).*(?=</span>)"))[-1:-2]
# hit
b4    <- str_split(b3, "hit\">")
hit   <- str_sub(sapply(b4, function(x){x[2]}), end=-8)
# <a class> 추출 확인
b5    <- b[which(str_detect(b, "subject_fixed"))-2]
# str(b5)
# href 가져오기
b6    <- str_sub(str_extract(b5, ("(?<=href=\").*(?=data-role)")), end=-4)
# str(b6)
# 글내용 접속 URL
url   <- paste0("https://www.clien.net", b6)
data  <- cbind(title, hit, url)
# data
final_data <- rbind(final_data, data)
# length(title)
# length(hit)
# length(url)
cat("\n", i)
}
dim(final_data)
head(final_data)
length(source)
str(source)
str(source)
length(source)
head(source, 10)
head(source, 100)
# str(source)
length(source)
str(b2)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
str(b2)
head(b2, 10)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
str(b2)
head(b2, 10)
}
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
head(b2, 10)
}
head(b2)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUR-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
head(b2, 10)
}
# str(b2)
head(b2, 10)
# str(b2)
b3<-repair_encoding(b2)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</span>)"))
# str(title)
}
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</strong>)"))
# str(title)
}
str(title)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="UTF-8")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
}
head(b2, 10)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUC-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
}
head(b2, 10)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUC-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</strong>)"))
}
str(title)
for(i in 1:10) {
path  <- "https://news.nate.com/recent?mid=n0100&type=c&date=20200409&page="
url   <- paste0(path, i)
b     <- readLines(url, encoding="EUC-KR")
# str(source)
length(source)
# head(source, 10)
# 게시판의 글 제목 우클릭-검사 하면
# <strong class="tit">'막말 제명'</strong>
b2    <- b[str_detect(b, "strong class=\"tit\"")]
# str(b2)
# b3<-repair_encoding(b2)
head(b2, 10)
title <- str_extract(b2, ("(?<=\">).*(?=</strong>)"))
str(title)
}
str(title)
title
library(rvest)
install.packages('rvest')
install.packages("rvest")
# 글 본문 데이터를 이용하여 형태소 분석 및 워드 클라우드
setwd("./")
final_data <- read.csv("./Data/final_data.csv", encoding="EUC-KR", fileEncoding="EUC-KR")
#final_data <- read.csv("./Data/final_data.csv", encoding="cp949", fileEncoding="UTF-8")
#final_data <- read.csv("./final_data.csv")
head(data)
install.packages("KoNLP")
install.packages("wordcloud")
library(KoNLP) # 한글 형태소 분석기
library(dplyr) # 데이터프레임 다루는 라이브러리
# useNIADic()  # 사전 데이터
library(stringr) # 텍스 처리 라이브러리
library(RColorBrewer) # 칼라 팔레트
library(wordcloud) # 워드클라우드 라이브러리
txt <- str_replace_all(final_data[,4], "\\W", " ")
str(txt)
nouns <- extractNoun(txt)
head(nouns)
wordcount <- table(unlist(nouns))
wordcount
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
str(df_word)
df_word <- rename(df_word, word = Var1, freq = Freq)
str(df_word)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
top_20
?brewer.pal
pal <- brewer.pal(8,"Dark2")
set.seed(1234)
wordcloud(words = df_word$word, # 단어
freq = df_word$freq, # 빈도
min.freq = 2, # 최소 단어 빈도
max.words = 200, # 표현 단어 수
random.order = F, # 고빈도 단어 중앙 배치
rot.per = .1, # 회전 단어 비율
scale = c(4, 0.3), # 단어 크기 범위
colors = pal) # 색깔 목록
?lm
lm
require(graphics)
## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1) # omitting intercept
anova(lm.D9)
summary(lm.D90)
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(lm.D9, las = 1)      # Residuals, Fitted, ...
par(opar)
y <-runif(100)
x1 <-runif(100)
x2 <-runif(100)
x3 <-runif(100)
x4 <-runif(100)
x5 <-runif(100)
x6 <-runif(100)
data <- cbind(y,x1,x2,x3,x4,x5,x6)
data2 <- data.frame(data)
head(data2)
fit<-lm(y~., data=data2) #y를 뺀 모두
anova(fit)
summary(fit)
head(data2)
anova(fit)
summary(fit)
## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1) # omitting intercept
anova(lm.D9)
summary(lm.D90)
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(lm.D9, las = 1)      # Residuals, Fitted, ...
par(opar)
y <-runif(100)
x1 <-runif(100)
x2 <-runif(100)
x3 <-runif(100)
x4 <-runif(100)
x5 <-runif(100)
x6 <-runif(100)
data <- cbind(y,x1,x2,x3,x4,x5,x6)
data2 <- data.frame(data)
head(data2)
fit<-lm(y~., data=data2) #y를 뺀 모두
anova(fit)
summary(fit)
require(graphics)
data(cars)
cars
fit <- lm(dist!.,data=cars)
anova(fit)
summary(fit)
fit$coefficients[2] # w
fit$coefficients[1] # b
cars
fit <- lm(dist~.,data=cars)
anova(fit)
summary(fit)
?anova(fit)
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
train <- read.csv('../input/train.csv', stringsAsFactors = F)
---
title: 'Exploring the Titanic Dataset'
author: 'Megan L. Risdal'
date: '6 March 2016'
output:
html_document:
number_sections: true
toc: true
fig_width: 7
fig_height: 4.5
theme: readable
highlight: tango
---
---
title: 'Exploring the Titanic Dataset'
author: 'Megan L. Risdal'
date: '6 March 2016'
output:
html_document:
number_sections: true
toc: true
fig_width: 7
fig_height: 4.5
theme: readable
highlight: tango
---
# Introduction
This is my first stab at a Kaggle script. I have chosen to work with the Titanic dataset after spending some time poking around on the site and looking at other scripts made by other Kagglers for inspiration. I will also focus on doing some illustrative data visualizations along the way. I'll then use `randomForest` to create a model predicting survival on the Titanic. I am new to machine learning and hoping to learn a lot, so feedback is very welcome!
There are three parts to my script as follows:
* Feature engineering
* Missing value imputation
* Prediction!
## Load and check data
```{r, message = FALSE}
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
library('randomForest') # classification algorithm
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
Now that our packages are loaded, let's read in and take a peek at the data.
```{r, message=FALSE, warning=FALSE}
train <- read.csv('../input/train.csv', stringsAsFactors = F)
test  <- read.csv('../input/test.csv', stringsAsFactors = F)
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
# Load packages
library('ggplot2') # visualization
#library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
# Load packages
library('ggplot2') # visualization
#library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
#library('mice') # imputation
library('randomForest') # classification algorithm
#library('mice') # imputation
library('randomForest') # classification algorithm
# Load packages
library('ggplot2') # visualization
#library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
#library('mice') # imputation
library('randomForest') # classification algorithm
Now that our packages are loaded, let's read in and take a peek at the data.
```{r, message=FALSE, warning=FALSE}
train <- read.csv('../input/train.csv', stringsAsFactors = F)
test  <- read.csv('../input/test.csv', stringsAsFactors = F)
full  <- bind_rows(train, test) # bind training & test data
# check data
str(full)

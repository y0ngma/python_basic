{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML package of PySpark\n",
    "최상단 레벨에 3개의 추상 클래스를 가지고 있다. (트랜스포머, 에스티메이터, 파이프라인)\n",
    "\n",
    "## 트랜스포머\n",
    "\n",
    "트랜스포머 클래스는 데이터프레임에 새로운 컬럼을 추가하고 데이터를 변형.\n",
    "spark.ml.feature에 많은 트랜스포머들이 있음.\n",
    "\n",
    "ChiSqSelector, CountVectorizer, HashingTF, IDF, indexToString, MaxAbsScaler, MinMaxScaler  \n",
    "import findspark,pyspark  \n",
    "findspark.find()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Bigdata\\\\spark-2.4.5-bin-hadoop2.7'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD로 바뀐것 ->DF로 다시 바꿔줘야 MLlib사용가능\n",
    "import findspark,pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=appName, master=local[*]) created by __init__ at <ipython-input-3-31414dea88fd>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-31414dea88fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'appName'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local[*]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=appName, master=local[*]) created by __init__ at <ipython-input-3-31414dea88fd>:5 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local[*]')\n",
    "sc=pyspark.SparkContext(conf=conf)\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(12,10,3), (1,4,2)],\n",
    "    ['a','b','c']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "| 12| 10|  3|\n",
      "|  1|  4|  2|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_77290a494e36"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.VectorAssembler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([12.0, 10.0, 3.0])),\n",
       " Row(features=DenseVector([1.0, 4.0, 2.0]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.VectorAssembler(inputCols=['a','b','c'], outputCol='features') \\\n",
    "                .transform(df) \\\n",
    "                .select('features') \\\n",
    "                .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "labels = [\n",
    "      ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n",
    "      ('BIRTH_PLACE', typ.StringType()),\n",
    "      ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "      ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "      ('CIG_BEFORE', typ.IntegerType()),\n",
    "      ('CIG_1_TRI', typ.IntegerType()),\n",
    "      ('CIG_2_TRI', typ.IntegerType()),\n",
    "      ('CIG_3_TRI', typ.IntegerType()),\n",
    "      ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "      ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "      ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "      ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "      ('DIABETES_PRE', typ.IntegerType()),\n",
    "      ('DIABETES_GEST', typ.IntegerType()),\n",
    "      ('HYP_TENS_PRE', typ.IntegerType()),\n",
    "      ('HYP_TENS_GEST', typ.IntegerType()),\n",
    "      ('PREV_BIRTH_PRETERM', typ.IntegerType())\n",
    "]\n",
    "schema = typ.StructType([typ.StructField(e[0],e[1],False) for e in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../data/RDD_example/\"\n",
    "births = spark.read.csv(base_path+'births_transformed.csv',header=True,schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIRTH_PLACE 칼럼 인코딩 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "births= births.withColumn('BIRTH_PLACE_INT', births['BIRTH_PLACE']\\\n",
    "                .cast(typ.IntegerType()))\n",
    "# BIRTH_PLACE_INT 라는 컬럼을 만든뒤, births['BIRTH_PLACE']를 가져와 cast한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=ft.OneHotEncoder(inputCol = 'BIRTH_PLACE_INT',\n",
    "                        outputCol = 'BIRTH_PLACE_VEC')\n",
    "\n",
    "# input, output을 지정하주고 onehotencode한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCreator=ft.VectorAssembler(\n",
    "                    inputCols=[col[0] for col in labels[2:]]+\\\n",
    "                    [encoder.getOutputCol()],\n",
    "                    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an estimator  \n",
    "에스티메이터  \n",
    "에스티메이터는 관찰된 데이터들에 대해 예측이나 분류를 수행하는데 필요한 통계모델  \n",
    "\n",
    "- 분류모델 : LogisticRegression,  DecisionTreeClassifier, RandomForestClassifier...\n",
    "- 회귀모델 : LinearRegression, RandomForestRegression...\n",
    "- 군집모델 : K-Means, GaussianMixture ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "logistic = cl.LogisticRegression(maxIter = 10, regParam=0.01,\n",
    "                                labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline\n",
    "\n",
    "파이프라인  \n",
    "ML에서의 파이프라인은 엔드 투 엔드 변환 - 추정 과정에 대한 처리.  \n",
    "원본 데이터를 받아서(데이터프레임) 필요한 데이터 변형을 수행 한 수(트랜스포메이션), 최종적으로  \n",
    "통계모델(에스티메이터)을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipline=Pipeline(stages=[encoder, featuresCreator, logistic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_train, births_test = births.randomSplit([0.7, 0.3], seed = 555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[INFANT_ALIVE_AT_REPORT: int, BIRTH_PLACE: string, MOTHER_AGE_YEARS: int, FATHER_COMBINED_AGE: int, CIG_BEFORE: int, CIG_1_TRI: int, CIG_2_TRI: int, CIG_3_TRI: int, MOTHER_HEIGHT_IN: int, MOTHER_PRE_WEIGHT: int, MOTHER_DELIVERY_WEIGHT: int, MOTHER_WEIGHT_GAIN: int, DIABETES_PRE: int, DIABETES_GEST: int, HYP_TENS_PRE: int, HYP_TENS_GEST: int, PREV_BIRTH_PRETERM: int, BIRTH_PLACE_INT: int]\n"
     ]
    }
   ],
   "source": [
    "print(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model=model.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=12, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=60, MOTHER_PRE_WEIGHT=154, MOTHER_DELIVERY_WEIGHT=154, MOTHER_WEIGHT_GAIN=0, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 12.0, 1: 99.0, 6: 60.0, 7: 154.0, 8: 154.0, 16: 1.0}), rawPrediction=DenseVector([1.1749, -1.1749]), probability=DenseVector([0.764, 0.236]), prediction=0.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rawPrediction : 회귀계수의 선형 결합값.  \n",
    "probability : 확률  \n",
    "prediction : 예측결과  \n",
    "\n",
    "# Model performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "            rawPredictionCol='probability',\n",
    "            labelCol='INFANT_ALIVE_AT_REPORT'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7401520968450921\n",
      "0.713487623217287\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test_model, {evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test_model, {evaluator.metricName:'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = './data/infant_oneHotEncoder_logistic_pipline'\n",
    "pipline.write().overwrite().save(pipelinePath)\n",
    "#아까 만들었던 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedPipeline = Pipeline.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=12, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=60, MOTHER_PRE_WEIGHT=154, MOTHER_DELIVERY_WEIGHT=154, MOTHER_WEIGHT_GAIN=0, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 12.0, 1: 99.0, 6: 60.0, 7: 154.0, 8: 154.0, 16: 1.0}), rawPrediction=DenseVector([1.1749, -1.1749]), probability=DenseVector([0.764, 0.236]), prediction=0.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedPipeline.fit(births_train).transform(births_test).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "modelPath = './data/infant_oneHotEncoder_logistic_pipelineModel'\n",
    "model.write().overwrite().save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedPipelineModel = PipelineModel.load(modelPath)\n",
    "test_loadedModel = loadedPipelineModel.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7401520968450921\n",
      "0.713487623217287\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test_loadedModel, {evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test_loadedModel, {evaluator.metricName:'areaUnderPR'}))\n",
    "# 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parmeter hyper-tuning\n",
    "\n",
    "Grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(labelCol = 'INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = tune.ParamGridBuilder().addGrid(logistic.maxIter, [2, 10, 50])\\\n",
    "                                .addGrid(logistic.regParam, [0.01, 0.05, 0.3])\\\n",
    "                                .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "rawPredictionCol='probability',\n",
    "labelCol = 'INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(\n",
    "estimator = logistic, \n",
    "estimatorParamMaps = grid,\n",
    "evaluator = evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [encoder, featuresCreator])\n",
    "data_transformer = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(data_transformer.transform(births_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_transformer.transform(births_test)\n",
    "results= cvModel.transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7410337059879951\n",
      "0.7147457221631203\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(results, {evaluator.metricName:'areaUnderROC'}))# test_loadedModel 0.7401520968450921\n",
    "print(evaluator.evaluate(results, {evaluator.metricName:'areaUnderPR'})) # test_loadedModel 0.713487623217287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    (\n",
    "        [\n",
    "            {key.name: paramValue}\n",
    "            for key, paramValue in zip(params.keys(), params.values())\n",
    "        ], metric\n",
    "    )\n",
    "    for params, metric in zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'maxIter': 50}, {'regParam': 0.01}], 0.7384482711319624)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(results, key = lambda el: el[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ft.ChiSqSelector(numTopFeatures=5,\n",
    "                           featuresCol = featuresCreator.getOutputCol(),\n",
    "                           outputCol = 'selectedFeatures',\n",
    "                           labelCol = 'INFANT_ALIVE_AT_REPORT')\n",
    "logistic = cl.LogisticRegression(\n",
    "labelCol = 'INFANT_ALIVE_AT_REPORT',\n",
    "featuresCol = 'selectedFeatures')\n",
    "pipeline = Pipeline(stages = [encoder, featuresCreator, selector])\n",
    "data_transformer = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = tune.TrainValidationSplit(\n",
    "estimator = logistic, \n",
    "estimatorParamMaps = grid,\n",
    "evaluator = evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsModel = tvs.fit(data_transformer.transform(births_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_transformer.transform(births_test)\n",
    "results = tvsModel.transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62100281440712\n",
      "0.5898593074787732\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(results, {evaluator.metricName:'areaUnderROC'}))# results기존값 0.7410337059879951\n",
    "print(evaluator.evaluate(results, {evaluator.metricName:'areaUnderPR'})) # results기존갑 0.7147457221631203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardizing continuous variables\n",
    "## 표준화하기\n",
    "- 연속변수 표준화는 핓처간의 관계를 더욱 잘 이해하게 할 뿐만아니라 컴퓨팅 계산량을 효율적으로 하기도 하면 수치적 에러가 일어나지 않게 하기도 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(0, 100)\n",
    "x = x/100.0 * np.pi * 4\n",
    "y = x * np.sin(x/1.764) + 20.1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = typ.StructType([\n",
    "    typ.StructField('continuous_var', typ.DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.createDataFrame([[float(e), ] for e in y], schema=schema)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(continuous_var=20.1234),\n",
       " Row(continuous_var=20.132344452369832),\n",
       " Row(continuous_var=20.159087064491775)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    continuous_var|\n",
      "+------------------+\n",
      "|           20.1234|\n",
      "|20.132344452369832|\n",
      "|20.159087064491775|\n",
      "+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = ft.VectorAssembler(\n",
    "inputCols = ['continuous_var'],\n",
    "outputCol = 'continuous_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "# MinMaxScaler\n",
    "# MaxabsScaler\n",
    "normalizer = ft.StandardScaler(\n",
    "                inputCol = vectorizer.getOutputCol(),\n",
    "                outputCol = 'normalized',\n",
    "                withMean = True,\n",
    "                withStd = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|    continuous_var|      continuous_vec|          normalized|\n",
      "+------------------+--------------------+--------------------+\n",
      "|           20.1234|           [20.1234]|[0.23429139554502...|\n",
      "|20.132344452369832|[20.132344452369832]|[0.23630959828688...|\n",
      "|20.159087064491775|[20.159087064491775]|[0.24234373105179...|\n",
      "|20.203356291885854|[20.203356291885854]|[0.2523325232564452]|\n",
      "| 20.26470185735763| [20.26470185735763]|[0.2661743755372584]|\n",
      "|20.342498180090526|[20.342498180090526]|[0.2837281334817457]|\n",
      "|  20.4359491438498|  [20.4359491438498]|[0.30481416351354...|\n",
      "|20.544094172020312|[20.544094172020312]|[0.32921572364798...|\n",
      "|20.665815568330437|[20.665815568330437]|[0.35668061983374...|\n",
      "|20.799847073505322|[20.799847073505322]|[0.38692313665363...|\n",
      "|  20.9447835797997|  [20.9447835797997]|[0.4196262292862522]|\n",
      "| 21.09909193743627| [21.09909193743627]|[0.4544439618423734]|\n",
      "|21.261122779470593|[21.261122779470593]|[0.4910041754963951]|\n",
      "| 21.42912328456607| [21.42912328456607]|[0.5289113682453727]|\n",
      "| 21.60125079063745| [21.60125079063745]|[0.5677497666557848]|\n",
      "|21.775587166351258|[21.775587166351258]| [0.607086568611146]|\n",
      "|21.950153842094366|[21.950153842094366]|[0.6464753348602804]|\n",
      "|22.122927397273514|[22.122927397273514]|[0.6854595060946465]|\n",
      "|22.291855596719525|[22.291855596719525]|[0.7235760213604691]|\n",
      "|22.454873765567744|[22.454873765567744]|[0.7603590128437555]|\n",
      "+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages = [vectorizer, normalizer])\n",
    "data_standardized = pipeline.fit(data).transform(data)\n",
    "data_standardized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier는 더블타임 데이터를 다룸\n",
    "import pyspark.sql.functions as func\n",
    "births = births.withColumn( 'INFANT_ALIVE_AT_REPORT', func.col('INFANT_ALIVE_AT_REPORT')\\\n",
    "                          .cast(typ.DoubleType()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_train, births_test = births.randomSplit( [0.7, 0.3], seed=333 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = cl.RandomForestClassifier( numTrees=5, maxDepth=5, \n",
    "                                        labelCol='INFANT_ALIVE_AT_REPORT' )\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7512530645009318\n",
      "0.7045194467626751\n"
     ]
    }
   ],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(labelCol = 'INFANT_ALIVE_AT_REPORT')\n",
    "print(evaluator.evaluate(test, {evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test, {evaluator.metricName:'areaUnderPR'}))\n",
    "# results기존값 0.7410337059879951\n",
    "# results기존갑 0.7147457221631203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = cl.DecisionTreeClassifier(maxDepth=5, \n",
    "                                       labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7030789615584603\n",
      "0.7083770130284276\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test, {evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test, {evaluator.metricName:'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.clustering as clus\n",
    "\n",
    "kmeans = clus.KMeans(k=5, featuresCol = 'features')\n",
    "pipeline = Pipeline(stages = [encoder, featuresCreator, kmeans])\n",
    "model = pipeline.fit(births_train)\n",
    "test = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=1, avg(MOTHER_HEIGHT_IN)=67.87682672233821, count(1)=479),\n",
       " Row(prediction=3, avg(MOTHER_HEIGHT_IN)=63.94812680115274, count(1)=9022),\n",
       " Row(prediction=4, avg(MOTHER_HEIGHT_IN)=66.14646464646465, count(1)=198),\n",
       " Row(prediction=2, avg(MOTHER_HEIGHT_IN)=85.08866995073892, count(1)=406),\n",
       " Row(prediction=0, avg(MOTHER_HEIGHT_IN)=65.39464033850494, count(1)=3545)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupBy('prediction').agg({'*': 'count', 'MOTHER_HEIGHT_IN': 'avg'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
